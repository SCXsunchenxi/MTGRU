{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import cPickle as pickle\n",
    "import h5py\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTGRU_AE(object):\n",
    "\n",
    "    def init_weights(self, input_dim, output_dim, name=None, std=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=std / math.sqrt(input_dim)), name=name)\n",
    "\n",
    "    def init_bias(self, output_dim, name=None):\n",
    "        return tf.Variable(tf.zeros([output_dim]), name=name)\n",
    "\n",
    "    def __init__(self, visit_num, visit_length, one_hot_input_dim, input_dim, info_dim, output_dim, output_dim1,\n",
    "                 output_dim2, output_dim3, voutput_dim, vhidden_dim, hidden_dim1, hidden_dim2, hidden_dim3,\n",
    "                 hidden_dim4,patient_visit_num,patient_visit_length,visit_time,visit_info,patient_num,code_inputindex,code_time):\n",
    "\n",
    "\n",
    "        # 一个病人visit的个数\n",
    "        self.visit_num = visit_num\n",
    "        # 一个visit中的code的个数\n",
    "        self.visit_length = visit_length\n",
    "\n",
    "        # 输入的one-hot的维度\n",
    "        self.one_hot_input_dim = one_hot_input_dim\n",
    "        # one-hot变换后作为网络输入的维度\n",
    "        self.input_dim = input_dim\n",
    "        # 患者信息\n",
    "        self.info_dim = info_dim\n",
    "        # 最后一层gru最终输出\n",
    "        self.output_dim = output_dim\n",
    "        # 中间层输出,每层gru有一个输出，visit层有一个输出\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "        self.output_dim3 = output_dim3\n",
    "        self.voutput_dim = voutput_dim\n",
    "        # 中间隐藏层，四个gru层，一个visit层\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim3 = hidden_dim3\n",
    "        self.hidden_dim4 = hidden_dim4\n",
    "        self.vhidden_dim = vhidden_dim\n",
    "        # one-hot后的矩阵参数\n",
    "        self.Wi = self.init_weights(one_hot_input_dim, input_dim, name='OneHot_w')\n",
    "        self.bi = self.init_bias(input_dim, name='OneHot_w')\n",
    "\n",
    "        # encoder的GRU参数\n",
    "        self.Wz_enc = self.init_weights(input_dim, hidden_dim1, name='Update_wx_enc')\n",
    "        self.Uz_enc = self.init_weights(hidden_dim1, hidden_dim1, name='Update_wh_enc')\n",
    "        self.bz_enc = self.init_bias(hidden_dim1, name='Update_bias_enc')\n",
    "\n",
    "        self.Wz_enc2 = self.init_weights(output_dim1, hidden_dim2, name='Update_wx_enc2')\n",
    "        self.Uz_enc2 = self.init_weights(hidden_dim2, hidden_dim2, name='Update_wh_enc2')\n",
    "        self.bz_enc2 = self.init_bias(hidden_dim2, name='Update_bias_enc2')\n",
    "\n",
    "        self.Wr_enc = self.init_weights(input_dim, hidden_dim1, name='Reset_wx_enc')\n",
    "        self.Ur_enc = self.init_weights(hidden_dim1, hidden_dim1, name='Reset_wh_enc')\n",
    "        self.br_enc = self.init_bias(hidden_dim1, name='Reset_bias_enc')\n",
    "\n",
    "        self.Wr_enc2 = self.init_weights(output_dim1, hidden_dim2, name='Reset_wx_enc2')\n",
    "        self.Ur_enc2 = self.init_weights(hidden_dim2, hidden_dim2, name='Reset_wh_enc2')\n",
    "        self.br_enc2 = self.init_bias(hidden_dim2, name='Reset_bias_enc2')\n",
    "\n",
    "        self.Wd_enc = tf.ones([1, self.hidden_dim1], dtype=tf.float32, name='Decay_w_enc')\n",
    "        self.Wd_enc2 = tf.ones([1, self.hidden_dim2], dtype=tf.float32, name='Decay_w_enc2')\n",
    "\n",
    "        self.Wh_enc = self.init_weights(self.input_dim, self.hidden_dim1, name='Canditateh_wx_enc')\n",
    "        self.Uh_enc = self.init_weights(self.hidden_dim1, self.hidden_dim1, name='Canditateh_wh_enc')\n",
    "        self.bh_enc = self.init_bias(self.hidden_dim1, name='Canditateh_bias_enc')\n",
    "\n",
    "        self.Wh_enc2 = self.init_weights(self.output_dim1, self.hidden_dim2, name='Canditateh_wx_enc2')\n",
    "        self.Uh_enc2 = self.init_weights(self.hidden_dim2, self.hidden_dim2, name='Canditateh_wh_enc2')\n",
    "        self.bh_enc2 = self.init_bias(self.hidden_dim2, name='Canditateh_bias_enc2')\n",
    "\n",
    "        # decoder的GRU参数\n",
    "        self.Wz_dec = self.init_weights(voutput_dim, hidden_dim3, name='Update_wx_dec')\n",
    "        self.Uz_dec = self.init_weights(hidden_dim3, hidden_dim3, name='Update_wh_dec')\n",
    "        self.bz_dec = self.init_bias(hidden_dim3, name='Update_bias_dec')\n",
    "\n",
    "        self.Wz_dec2 = self.init_weights(output_dim3, hidden_dim4, name='Update_wx_dec2')\n",
    "        self.Uz_dec2 = self.init_weights(hidden_dim4, hidden_dim4, name='Update_wh_dec2')\n",
    "        self.bz_dec2 = self.init_bias(hidden_dim4, name='Update_bias_dec2')\n",
    "\n",
    "        self.Wr_dec = self.init_weights(voutput_dim, hidden_dim3, name='Reset_wx_dec')\n",
    "        self.Ur_dec = self.init_weights(hidden_dim3, hidden_dim3, name='Reset_wh_dec')\n",
    "        self.br_dec = self.init_bias(hidden_dim3, name='Reset_bias_dec')\n",
    "\n",
    "        self.Wr_dec2 = self.init_weights(output_dim3, hidden_dim4, name='Reset_wx_dec2')\n",
    "        self.Ur_dec2 = self.init_weights(hidden_dim4, hidden_dim4, name='Reset_wh_dec2')\n",
    "        self.br_dec2 = self.init_bias(hidden_dim4, name='Reset_bias_dec2')\n",
    "\n",
    "        self.Wd_dec = tf.ones([1, self.hidden_dim3], dtype=tf.float32, name='Decay_w_dec')\n",
    "        self.Wd_dec2 = tf.ones([1, self.hidden_dim4], dtype=tf.float32, name='Decay_w_dec2')\n",
    "\n",
    "        self.Wh_dec = self.init_weights(self.voutput_dim, self.hidden_dim3, name='Canditateh_wx_dec')\n",
    "        self.Uh_dec = self.init_weights(self.hidden_dim3, self.hidden_dim3, name='Canditateh_wh_dec')\n",
    "        self.bh_dec = self.init_bias(self.hidden_dim3, name='Canditateh_bias_dec')\n",
    "\n",
    "        self.Wh_dec2 = self.init_weights(self.output_dim3, self.hidden_dim4, name='Canditateh_wx_dec')\n",
    "        self.Uh_dec2 = self.init_weights(self.hidden_dim4, self.hidden_dim4, name='Canditateh_wh_dec')\n",
    "        self.bh_dec2 = self.init_bias(self.hidden_dim4, name='Canditateh_bias_dec')\n",
    "\n",
    "        # visit层的参数\n",
    "        # 输入是上一层的输出加上病人信息\n",
    "        self.Wz_v = self.init_weights(output_dim2 + info_dim, vhidden_dim, name='Update_wx_v')\n",
    "        self.Uz_v = self.init_weights(vhidden_dim, vhidden_dim, name='Update_wh_v')\n",
    "        self.bz_v = self.init_bias(vhidden_dim, name='Update_bias_dec')\n",
    "\n",
    "        self.Wr_v = self.init_weights(output_dim2 + info_dim, vhidden_dim, name='Reset_wx_v')\n",
    "        self.Ur_v = self.init_weights(vhidden_dim, vhidden_dim, name='Reset_wh_v')\n",
    "        self.br_v = self.init_bias(vhidden_dim, name='Reset_bias_v')\n",
    "\n",
    "        self.Wd_v = tf.ones([1, self.vhidden_dim], dtype=tf.float32, name='Decay_w_v')\n",
    "\n",
    "        self.Wh_v = self.init_weights(self.output_dim2 + info_dim, self.vhidden_dim, name='Canditateh_wx_v')\n",
    "        self.Uh_v = self.init_weights(self.vhidden_dim, self.vhidden_dim, name='Canditateh_wh_v')\n",
    "        self.bh_v = self.init_bias(self.vhidden_dim, name='Canditateh_bias_v')\n",
    "\n",
    "        # 输出层\n",
    "        # visit层的输出\n",
    "        self.Wov = self.init_weights(vhidden_dim, voutput_dim, name='Visit_weight')\n",
    "        self.bov = self.init_bias(voutput_dim, name='Visit_bias')\n",
    "\n",
    "        # encoder1\n",
    "        self.Wo1 = self.init_weights(hidden_dim1, output_dim1, name='Output1_weight')\n",
    "        self.bo1 = self.init_bias(output_dim1, name='Output1_bias')\n",
    "\n",
    "        # encoder2\n",
    "        self.Wo2 = self.init_weights(hidden_dim2, output_dim2, name='Output2_weight')\n",
    "        self.bo2 = self.init_bias(output_dim2, name='Output2_bias')\n",
    "\n",
    "        # decoder1\n",
    "        self.Wo3 = self.init_weights(hidden_dim3, output_dim3, name='Output3_weight')\n",
    "        self.bo3 = self.init_bias(output_dim3, name='Output3_bias')\n",
    "\n",
    "        # 最终输出(decoder2)\n",
    "        self.Wo = self.init_weights(hidden_dim4, output_dim, name='Output_w')\n",
    "        self.bo = self.init_bias(output_dim, name='Output_bias')\n",
    "\n",
    "        # 输入占位符\n",
    "        # [batch size x seq length x input dim]\n",
    "        self.inputindex=tf.placeholder(dtype=tf.int32,shape=[None,None])\n",
    "        self.one_hot_input=tf.one_hot(indices=self.inputindex, depth=self.one_hot_input_dim, axis=2)\n",
    "        #self.one_hot_input = tf.placeholder('float', shape=[None, None, self.one_hot_input_dim])\n",
    "        self.time = tf.placeholder('float', [None, None])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        # 患者人口信息\n",
    "        self.info = tf.placeholder('float', shape=[None, None, self.info_dim])\n",
    "\n",
    "        # 为了获得所有patien的visit和 patient向量，输入全部信息\n",
    "        self.patient_visit_num = patient_visit_num # 一个患者的visit个数\n",
    "        self.patient_visit_length = patient_visit_length # 一个患者的visit中的code个数\n",
    "        self.visit_time = visit_time\n",
    "        self.visit_info = visit_info\n",
    "        self.patient_num=patient_num\n",
    "        self.code_inputindex=code_inputindex #一个病人所有code的index\n",
    "        self.code_time=code_time\n",
    "\n",
    "\n",
    "\n",
    "    # 对输入的one-hot使用矩阵处理，code的嵌入向量1\n",
    "    def get_input(self, one_hot_input):\n",
    "        input = tf.matmul(one_hot_input, self.Wi) + self.bi\n",
    "        return input\n",
    "\n",
    "    # encoder第一层TGRU cell，code的嵌入向量2\n",
    "    def TGRU_encoder_cell1(self, prev_h, concat_input):\n",
    "\n",
    "        # concat_input:[batch_size x input_dim+1]\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "        x = tf.slice(concat_input, [0, 1], [batch_size, self.input_dim])\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "\n",
    "        ft = self.map_elapse_time(t, self.hidden_dim1)\n",
    "\n",
    "        z = tf.sigmoid(tf.matmul(x, self.Wz_enc) + tf.matmul(prev_h, self.Uz_enc) + self.bz_enc)\n",
    "        r = tf.sigmoid(tf.matmul(x, self.Wr_enc) + tf.matmul(prev_h, self.Ur_enc) + self.br_enc)\n",
    "        d = tf.matmul(ft, self.Wd_enc)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(x, self.Wh_enc) + tf.matmul(r * h_, self.Uh_enc) + self.bh_enc)\n",
    "\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    # encoder第二层TGRU cell\n",
    "    def TGRU_encoder_cell2(self, prev_h, concat_input):\n",
    "\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "        x = tf.slice(concat_input, [0, 1], [batch_size, self.output_dim1])\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "\n",
    "        ft = self.map_elapse_time(t, self.hidden_dim2)\n",
    "\n",
    "        z = tf.sigmoid(tf.matmul(x, self.Wz_enc2) + tf.matmul(prev_h, self.Uz_enc2) + self.bz_enc2)\n",
    "        r = tf.sigmoid(tf.matmul(x, self.Wr_enc2) + tf.matmul(prev_h, self.Ur_enc2) + self.br_enc2)\n",
    "        d = tf.matmul(ft, self.Wd_enc2)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(x, self.Wh_enc2) + tf.matmul(r * h_, self.Uh_enc2) + self.bh_enc2)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    # visit层的TGURU cell\n",
    "    # 此时的输入与其他cell的不同，concat_input应该是多个时刻的code的，而其他的输入都是一个时刻的code，因此在后面对get_encoder_h2输出的结果做处理\n",
    "    # 此时的concat_input是一个三维向量，也就是有多时刻的code，每个时刻有一个时间和batch个数个code\n",
    "    # [visit_length x batch_size x 1+input_dim ]\n",
    "    # 创建每个visit的长度的参数，来方便创建多个输入x\n",
    "    def TGRU_visit_cell(self, prev_h, concat_input):\n",
    "\n",
    "        # visit_length x batch_size x input_dim+info_dim+info_dim+1\n",
    "        batch_size = tf.shape(concat_input)[1]\n",
    "\n",
    "        # 只获取第一个时间\n",
    "        t = tf.slice(concat_input, [0, 0, 0], [1, batch_size, 1])\n",
    "        # 只获取第一个info\n",
    "        info = tf.slice(concat_input, [0, 0, 1], [1, batch_size, self.info_dim])\n",
    "\n",
    "        # 循环创建局部变量\n",
    "        for i in range(self.visit_length):\n",
    "            locals()['x' + str(i)] = tf.slice(concat_input, [i, 0, 1 + self.info_dim],\n",
    "                                              [1, batch_size, self.output_dim2])\n",
    "\n",
    "        # 获得visit层的输入\n",
    "        visit_x = tf.zeros([batch_size, self.output_dim2], dtype=tf.float32)\n",
    "        for i in range(self.visit_length):\n",
    "            visit_x += locals()['x' + str(i)]\n",
    "\n",
    "        visit_x = visit_x / self.visit_length\n",
    "\n",
    "        # 把输入和病人信息放在一个向量中\n",
    "        visit_x = tf.concat([visit_x, info], 2)\n",
    "        # 3维变2维 第一维是大小是1，没用\n",
    "        visit_x = tf.reshape(visit_x,[tf.shape(visit_x)[1],tf.shape(visit_x)[2]])\n",
    "        t = tf.reshape(t,[tf.shape(t)[1],tf.shape(t)[2]])\n",
    "\n",
    "\n",
    "        # visit嵌入向量经过GRU输出\n",
    "        ft = self.map_elapse_time(t, self.vhidden_dim)\n",
    "        z = tf.sigmoid(tf.matmul(visit_x, self.Wz_v) + tf.matmul(prev_h, self.Uz_v) + self.bz_v)\n",
    "        r = tf.sigmoid(tf.matmul(visit_x, self.Wr_v) + tf.matmul(prev_h, self.Ur_v) + self.br_v)\n",
    "        d = tf.matmul(ft, self.Wd_v)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(visit_x, self.Wh_v) + tf.matmul(r * h_, self.Uh_v) + self.bh_v)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    # decoder第一层TGRU cell\n",
    "    def TGRU_decoder_cell1(self, prev_h, concat_input):\n",
    "\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "        x = tf.slice(concat_input, [0, 1], [batch_size, self.voutput_dim])\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "\n",
    "        ft = self.map_elapse_time(t, self.hidden_dim3)\n",
    "\n",
    "        z = tf.sigmoid(tf.matmul(x, self.Wz_dec) + tf.matmul(prev_h, self.Uz_dec) + self.bz_dec)\n",
    "        r = tf.sigmoid(tf.matmul(x, self.Wr_dec) + tf.matmul(prev_h, self.Ur_dec) + self.br_dec)\n",
    "        d = tf.matmul(ft, self.Wd_dec)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(x, self.Wh_dec) + tf.matmul(r * h_, self.Uh_dec) + self.bh_dec)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    # decoder第二层TGRU cell\n",
    "    def TGRU_decoder_cell2(self, prev_h, concat_input):\n",
    "\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "        x = tf.slice(concat_input, [0, 1], [batch_size, self.output_dim3])\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "\n",
    "        ft = self.map_elapse_time(t, self.hidden_dim4)\n",
    "\n",
    "        z = tf.sigmoid(tf.matmul(x, self.Wz_dec2) + tf.matmul(prev_h, self.Uz_dec2) + self.bz_dec2)\n",
    "        r = tf.sigmoid(tf.matmul(x, self.Wr_dec2) + tf.matmul(prev_h, self.Ur_dec2) + self.br_dec2)\n",
    "        d = tf.matmul(ft, self.Wd_dec2)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(x, self.Wh_dec2) + tf.matmul(r * h_, self.Uh_dec2) + self.bh_dec2)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    # 每个输出层的操作\n",
    "    # encoder1的输出\n",
    "    def get_output1(self, h):\n",
    "        output = tf.matmul(h, self.Wo1) + self.bo1\n",
    "        # output = tf.nn.softmax(tf.nn.relu(tf.matmul(state, self.Wo) + self.bo))\n",
    "        return output\n",
    "\n",
    "    # encoder2的输出\n",
    "    def get_output2(self, h):\n",
    "        output = tf.matmul(h, self.Wo2) + self.bo2\n",
    "        # output = tf.nn.softmax(tf.nn.relu(tf.matmul(state, self.Wo) + self.bo))\n",
    "        return output\n",
    "\n",
    "    # visit的输出\n",
    "    def get_outputv(self, h):\n",
    "        output = tf.matmul(h, self.Wov) + self.bov\n",
    "        # output = tf.nn.softmax(tf.nn.relu(tf.matmul(state, self.Wo) + self.bo))\n",
    "        return output\n",
    "\n",
    "    # decoder1的输出\n",
    "    def get_output3(self, h):\n",
    "        output = tf.matmul(h, self.Wo3) + self.bo3\n",
    "        # output = tf.nn.softmax(tf.nn.relu(tf.matmul(state, self.Wo) + self.bo))\n",
    "        return output\n",
    "\n",
    "    # 最终输出（decoder2的输出）\n",
    "    def get_output(self, h):\n",
    "        output = tf.matmul(h, self.Wo) + self.bo\n",
    "        # output = tf.nn.softmax(tf.nn.relu(tf.matmul(state, self.Wo) + self.bo))\n",
    "        return output\n",
    "\n",
    "    # encoder1的输出h\n",
    "    def get_encoder1_h(self):  # Returns all hidden states for the samples in a batch\n",
    "\n",
    "        convert_input = self.get_input(self.one_hot_input)\n",
    "        batch_size = tf.shape(convert_input)[0]\n",
    "        scan_input = tf.transpose(convert_input, perm=[1, 0, 2])  # scan input is [seq_length x batch_size x input_dim]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim1], tf.float32)\n",
    "\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        concat_input = tf.concat([scan_time, scan_input], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder1_h = tf.scan(self.TGRU_encoder_cell1, concat_input, initializer=initial_hidden, name='encoder1_h')\n",
    "\n",
    "        return encoder1_h\n",
    "\n",
    "    # encoder2的输出h\n",
    "    def get_encoder2_h(self):  # Returns all hidden states for the samples in a batch\n",
    "\n",
    "        encoder1_h = self.get_encoder1_h()\n",
    "        encoder1_output = tf.map_fn(self.get_output1, encoder1_h)\n",
    "\n",
    "        batch_size = tf.shape(encoder1_h)[1]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim2], tf.float32)\n",
    "\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        concat_input = tf.concat([scan_time, encoder1_output], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder2_h = tf.scan(self.TGRU_encoder_cell2, concat_input, initializer=initial_hidden, name='encoder2_h')\n",
    "        return encoder2_h\n",
    "\n",
    "    # visit的输出h\n",
    "    def get_visit_h(self):  # Returns all hidden states for the samples in a batch\n",
    "\n",
    "        encoder2_h = self.get_encoder2_h()\n",
    "        encoder2_output = tf.map_fn(self.get_output2, encoder2_h)\n",
    "\n",
    "        batch_size = tf.shape(encoder2_h)[1]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "\n",
    "        # make info [batch_size x seq_length x info_dim] --> [seq_length x batch_size x info_dim]\n",
    "        scan_info = tf.transpose(self.info, [1, 0, 2])\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "\n",
    "        # [seq_length x batch_size x input_dim+info_dim+1]\n",
    "        concat_input = tf.concat([scan_time, scan_info, encoder2_output], 2)\n",
    "\n",
    "        # 已经知道了visit的个数和长度，可以直接把concat_input拆分成visit个\n",
    "        # [visit_num x visit_length x batch_size x input_dim+info_dim_1]\n",
    "        initial_hidden = tf.zeros([ batch_size, self.vhidden_dim], tf.float32)\n",
    "        concat_input = tf.reshape(concat_input, [self.visit_num, self.visit_length, tf.shape(concat_input)[1],\n",
    "                                                 tf.shape(concat_input)[2]])\n",
    "        visit_h = tf.scan(self.TGRU_visit_cell, concat_input, initializer=initial_hidden, name='visit')\n",
    "\n",
    "        return visit_h\n",
    "\n",
    "\n",
    "\n",
    "    # 获得visit和patient的表达\n",
    "    def get_patient_vector(self):\n",
    "        visit_h = self.get_visit_vector()\n",
    "        # 最后一组h，把h数组反向取第一组\n",
    "        pv = tf.reverse(visit_h,[0])[0, :, :]\n",
    "        return visit_h,pv\n",
    "\n",
    "    # 得到decoder第1层的h\n",
    "    # 此时的输入是visit的h经过output后的结果,，要注意对一个visit的output展开成这个visit的code个\n",
    "    def get_decoder1_h(self):\n",
    "        visit_h = self.get_visit_h()\n",
    "        visit_output_ = tf.map_fn(self.get_outputv, visit_h)\n",
    "        # 对输入展开\n",
    "        visit_output = []\n",
    "        for i in range(self.visit_num):\n",
    "            for j in range(self.visit_length):\n",
    "                visit_output.append(visit_output_[i])\n",
    "\n",
    "        batch_size = tf.shape(visit_h)[1]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim3], tf.float32)\n",
    "\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        concat_input = tf.concat([scan_time, visit_output], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        decoder1_h = tf.scan(self.TGRU_decoder_cell1, concat_input, initializer=initial_hidden, name='decoder1_h')\n",
    "        return decoder1_h\n",
    "\n",
    "    # decoder2的输出h\n",
    "    def get_decoder2_h(self):  # Returns all hidden states for the samples in a batch\n",
    "\n",
    "        decoder1_h = self.get_decoder1_h()\n",
    "        decoder1_output = tf.map_fn(self.get_output3, decoder1_h)\n",
    "\n",
    "        batch_size = tf.shape(decoder1_h)[1]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim4], tf.float32)\n",
    "\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        concat_input = tf.concat([scan_time, decoder1_output], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        decoder2_h = tf.scan(self.TGRU_decoder_cell2, concat_input, initializer=initial_hidden, name='decoder2_h')\n",
    "        return decoder2_h\n",
    "\n",
    "    # 获得整个AE的输出\n",
    "    def get_decoder_outputs(self):  # Returns the output of only the last time step\n",
    "        decoder2_h = self.get_decoder2_h()\n",
    "        all_outputs = tf.map_fn(self.get_output, decoder2_h)\n",
    "        outputs = tf.transpose(all_outputs, perm=[1, 0, 2])\n",
    "        return outputs\n",
    "\n",
    "    # 获得预测和输入的距离使用MSE\n",
    "    def get_reconstruction_loss(self):\n",
    "        outputs = self.get_decoder_outputs()\n",
    "        loss = tf.reduce_mean(tf.square(self.one_hot_input - outputs))\n",
    "        return loss\n",
    "\n",
    "    def get_cross_loss(self):\n",
    "        outputs = self.get_decoder_outputs()\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.one_hot_input, logits=outputs))\n",
    "        return cross_entropy\n",
    "\n",
    "    def map_elapse_time(self, t, dim):\n",
    "        c1 = tf.constant(1, dtype=tf.float32)\n",
    "        c2 = tf.constant(2.7183, dtype=tf.float32)\n",
    "\n",
    "        T = tf.div(c1, tf.log(t + c2), name='Log_elapse_time')\n",
    "        # T = tf.div(c1, tf.add(t , c1), name='Log_elapse_time')\n",
    "\n",
    "        return T\n",
    "\n",
    "    # 获取code，visit，patient的表达\n",
    "    def get_c_v_p(self):\n",
    "        # code的最终向量是one-hot 变换w变换后的值\n",
    "        code_vector = self.get_input(self.one_hot_input)\n",
    "        # visit的向量是visit层的h\n",
    "        visit_vector = self.get_visit_h()\n",
    "        # patient的向量是最后一个visit的h\n",
    "        patient_vector = self.get_patient_vector()\n",
    "        return code_vector, visit_vector, patient_vector\n",
    "\n",
    "\n",
    "\n",
    "# 获取数据集中每个病人的visit的表达和patient表达\n",
    "    def TGRU_visit_cell2(self, prev_h, concat_input):\n",
    "        # batch_size x input_dim+info_dim+info_dim+1\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "\n",
    "        # 时间\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "        # visit_info\n",
    "        info = tf.slice(concat_input, [0, 1], [batch_size, self.info_dim])\n",
    "        # 输入\n",
    "        x = tf.slice(concat_input, [0, 1+self.info_dim], [batch_size, self.output_dim2])\n",
    "\n",
    "        # 把输入和病人信息放在一个向量中\n",
    "        visit_x = tf.concat([x, info], 1)\n",
    "\n",
    "        # visit嵌入向量经过GRU输出\n",
    "        ft = self.map_elapse_time(t, self.vhidden_dim)\n",
    "        z = tf.sigmoid(tf.matmul(visit_x, self.Wz_v) + tf.matmul(prev_h, self.Uz_v) + self.bz_v)\n",
    "        r = tf.sigmoid(tf.matmul(visit_x, self.Wr_v) + tf.matmul(prev_h, self.Ur_v) + self.br_v)\n",
    "        d = tf.matmul(ft, self.Wd_v)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(visit_x, self.Wh_v) + tf.matmul(r * h_, self.Uh_v) + self.bh_v)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "    # 备注的矩阵大小是假设一个用户的code有39个，2个visit，总共的code种类259个\n",
    "    def get_encoder1_h_forVector(self,no):  # Returns all hidden states for the samples in a batch\n",
    "        convert_input = self.get_input(tf.one_hot(indices=self.code_inputindex[no], depth=self.one_hot_input_dim,axis=1))\n",
    "        batch_size = 1\n",
    "\n",
    "        # (39, 1, 259)\n",
    "        scan_input = tf.transpose([convert_input], perm=[1, 0, 2])  # scan input is [seq_length x batch_size x input_dim]\n",
    "        # (39, 1)\n",
    "        scan_time = tf.transpose([tf.to_float(self.code_time[no])])  # scan_time [seq_length x batch_size]\n",
    "        # (1, 150)\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim1], tf.float32)\n",
    "        # (39, 1, 1) make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        # (39, 1, 260)\n",
    "        concat_input = tf.concat([scan_time, scan_input], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder1_h = tf.scan(self.TGRU_encoder_cell1, concat_input, initializer=initial_hidden, name='encoder1_h_forVector')\n",
    "        return encoder1_h\n",
    "\n",
    "    def get_encoder2_h_forVector(self,no):  # Returns all hidden states for the samples in a batch\n",
    "        # (39, 1, 150)\n",
    "        encoder1_h = self.get_encoder1_h_forVector(no)\n",
    "        # (39, 1, 150)\n",
    "        encoder1_output = tf.map_fn(self.get_output1, encoder1_h)\n",
    "        batch_size = 1\n",
    "        # (39, 1)\n",
    "        scan_time = tf.transpose([tf.to_float(self.code_time[no])])  # scan_time [seq_length x batch_size]\n",
    "        # (1, 150)\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim2], tf.float32)\n",
    "        # (39, 1, 1) # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        # (39, 1, 151)\n",
    "        concat_input = tf.concat([scan_time, encoder1_output], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder2_h = tf.scan(self.TGRU_encoder_cell2, concat_input, initializer=initial_hidden, name='encoder2_h_forVector')\n",
    "        return encoder2_h\n",
    "\n",
    "    def get_one_visit_vector(self,no):\n",
    "        encoder2_h = self.get_encoder2_h_forVector(no)\n",
    "        encoder2_output = tf.map_fn(self.get_output2, encoder2_h)\n",
    "        batch_size= 1\n",
    "        n = 0\n",
    "        x=[]\n",
    "        for i in range(self.patient_visit_num[no][0]):\n",
    "            #(1, 150)\n",
    "            visit_input=tf.zeros([batch_size, self.output_dim2], dtype=tf.float32)\n",
    "            for j in range(self.patient_visit_length[no][i]):\n",
    "                visit_input+=encoder2_output[n]\n",
    "                n=n+1\n",
    "            x.append(visit_input/self.patient_visit_length[no][i])\n",
    "        # (2, 1)\n",
    "        scan_time = tf.transpose([tf.to_float(self.visit_time[no])])  # scan_time [seq_length x batch_size]\n",
    "        # (2, 1, 2) make info [batch_size x seq_length x info_dim] --> [seq_length x batch_size x info_dim]\n",
    "        scan_info = tf.transpose(tf.to_float([self.visit_info[no]]), [1, 0, 2])\n",
    "        # (2, 1, 1)make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        # (2, 1, 153)[seq_length x batch_size x input_dim+info_dim+1]\n",
    "        concat_input = tf.concat([scan_time, scan_info, x], 2)\n",
    "        # (1,100)\n",
    "        initial_hidden = tf.zeros([batch_size, self.vhidden_dim], tf.float32)\n",
    "\n",
    "        visit_vector = tf.scan(self.TGRU_visit_cell2, concat_input, initializer=initial_hidden, name='visit')\n",
    "        patient_vector = tf.reverse(visit_vector,[0])[0, :, :]\n",
    "        return visit_vector,patient_vector\n",
    "\n",
    "    def get_all_visit_vector(self):\n",
    "        visits_vector=[]\n",
    "        patients_vector=[]\n",
    "        for no in range(self.patient_num):\n",
    "            visit_vector, patient_vector=self.get_one_visit_vector(no)\n",
    "            visits_vector.append(visit_vector)\n",
    "            patients_vector.append(patient_vector)\n",
    "        return visits_vector,patients_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch生成器\n",
    "# bacth中有cutting和padding，由于长度不同，因此开始学习的时候可以每个数据单独学习\n",
    "# 生成的batch里的病人visit和code的个数一样\n",
    "def batch_generator(outFile, visit_num, code_num, patient_num, batch_size):\n",
    "    patient_code_file=open(outFile + '/patient_code' + '.seqs','rb')\n",
    "    patient_code=pickle.load(patient_code_file)\n",
    "    \n",
    "    codespatientsinfo_file=open(outFile + '/codespatientsinfo' + '.seqs','rb')\n",
    "    codespatientsinfo=pickle.load(codespatientsinfo_file)\n",
    "    \n",
    "    visit_delt_dates_file=open(outFile + '/visit_delt_dates' + '.seqs','rb')\n",
    "    visit_delt_dates=pickle.load(visit_delt_dates_file)\n",
    "    \n",
    "    code_delt_dates_file=open(outFile + '/code_delt_dates' + '.seqs','rb')\n",
    "    code_delt_dates=pickle.load(code_delt_dates_file)\n",
    "    \n",
    "    visits_num_file=open(outFile + '/visits_num' + '.seqs','rb')\n",
    "    visits_num=pickle.load(visits_num_file)\n",
    "    \n",
    "    codes_num_file=open(outFile + '/codes_num' + '.seqs','rb')\n",
    "    codes_num=pickle.load(codes_num_file)\n",
    "    \n",
    "    patient_code_file.close()\n",
    "    codespatientsinfo_file.close()\n",
    "    visit_delt_dates_file.close()\n",
    "    code_delt_dates_file.close()\n",
    "    visits_num_file.close()\n",
    "    codes_num_file.close()\n",
    "\n",
    "    batch_patient_code=[]\n",
    "    batch_code_delt_dates=[]\n",
    "    batch_codespatientsinfo=[]\n",
    "    batch_visit_delt_dates=[]\n",
    "\n",
    "    \n",
    "    # padding and cutting\n",
    "    for i in range(batch_size):\n",
    "        j=random.randint(0, patient_num-1)\n",
    "        if visits_num[j][0]== visit_num:\n",
    "           # print 'pick',j\n",
    "            code=[]\n",
    "            code_date=[]\n",
    "            code_info=[]\n",
    "            for k in range(visit_num):\n",
    "                if  codes_num[j][k]> code_num:\n",
    "                    if k==0: \n",
    "                        code.extend(patient_code[j][0:code_num])\n",
    "                        code_date.extend(code_delt_dates[j][0:code_num])\n",
    "                        code_info.extend(codespatientsinfo[j][0:code_num])\n",
    "                    else:\n",
    "                        start=k*codes_num[j][k-1]\n",
    "                        code.extend(patient_code[j][start :start+code_num])\n",
    "                        code_date.extend(code_delt_dates[j][start:start+code_num])\n",
    "                        code_info.extend(codespatientsinfo[j][start:start+code_num])\n",
    "                elif codes_num[j][k] < code_num:\n",
    "                    if k==0:\n",
    "                        code.extend(patient_code[j][:codes_num[j][k]])\n",
    "                        code.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_date.extend(code_delt_dates[j][:codes_num[j][k]])\n",
    "                        code_date.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_info.extend(codespatientsinfo[j][:codes_num[j][k]])\n",
    "                        code_info.extend([[0,0]]*(code_num-codes_num[j][k]))\n",
    "                    else:\n",
    "                        start2=0\n",
    "                        for n in range(k):\n",
    "                            start2+=codes_num[j][n]\n",
    "                        code.extend(patient_code[j][start2: start2+codes_num[j][k]])\n",
    "                        code.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_date.extend(code_delt_dates[j][start2:start2+codes_num[j][k]])\n",
    "                        code_date.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_info.extend(codespatientsinfo[j][start2:start2+codes_num[j][k]])\n",
    "                        code_info.extend([[0,0]]*(code_num-codes_num[j][k]))\n",
    "                else:\n",
    "                    code.extend(patient_code[j])\n",
    "                    code_date.extend(code_delt_dates[j])\n",
    "                    code_info.extend(codespatientsinfo[j])\n",
    "            batch_patient_code.append(code)\n",
    "            batch_code_delt_dates.append(code_date)  \n",
    "            batch_codespatientsinfo.append(code_info)\n",
    "                    \n",
    "        \n",
    "        # 考虑随机生成的时候可能有的数据始终选不到，因此还要有一个顺序生成\n",
    "    return batch_patient_code, batch_codespatientsinfo, batch_code_delt_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "learning_rate = 1e-3\n",
    "iters = 20\n",
    "\n",
    "# 网络参数\n",
    "info_dim=2\n",
    "one_hot_input_dim=259 # 数据len(types)=259, one-hot的长度应该是259\n",
    "input_dim = 200 \n",
    "hidden_dim1 =150\n",
    "output_dim1=150\n",
    "hidden_dim2=150\n",
    "output_dim2=150\n",
    "vhidden_dim=100\n",
    "voutput_dim=150\n",
    "hidden_dim3=150\n",
    "output_dim3=150\n",
    "hidden_dim4=150\n",
    "output_dim=259\n",
    "\n",
    "# 生成batch数据参数\n",
    "outFile='SEQ'\n",
    "checkpoint_dir='MODEL'\n",
    "result_dir='RESULT'\n",
    "batch_size=4\n",
    "visit_num=2 #visit_num=random.randint() \n",
    "code_num=10 #code_num=random.randint() # code_num=visit_length\n",
    "patient_num=14 #病人的总个数\n",
    "\n",
    "# 数据集获取\n",
    "patient_code_file=open(outFile + '/patient_code' + '.seqs','rb')\n",
    "patient_code=pickle.load(patient_code_file)\n",
    "\n",
    "code_delt_dates_file=open(outFile + '/code_delt_dates' + '.seqs','rb')\n",
    "code_delt_dates=pickle.load(code_delt_dates_file)\n",
    "\n",
    "visit_delt_dates_file=open(outFile + '/visit_delt_dates' + '.seqs','rb')\n",
    "visit_delt_dates=pickle.load(visit_delt_dates_file)\n",
    "\n",
    "visits_num_file=open(outFile + '/visits_num' + '.seqs','rb')\n",
    "visits_num=pickle.load(visits_num_file)\n",
    "\n",
    "codes_num_file=open(outFile + '/codes_num' + '.seqs','rb')\n",
    "codes_num=pickle.load(codes_num_file)\n",
    "\n",
    "visitspatientsinfo_file=open(outFile + '/visitspatientsinfo' + '.seqs','rb')\n",
    "visitspatientsinfo=pickle.load(visitspatientsinfo_file)\n",
    "\n",
    "codespatientsinfo_file=open(outFile + '/codespatientsinfo' + '.seqs','rb')\n",
    "codespatientsinfo=pickle.load(codespatientsinfo_file)\n",
    "\n",
    "patient_code_file.close()\n",
    "code_delt_dates_file.close()\n",
    "visit_delt_dates_file.close()\n",
    "visits_num_file.close()\n",
    "codes_num_file.close()\n",
    "visitspatientsinfo_file.close()\n",
    "codespatientsinfo_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化网络\n",
    "mtgruae = MTGRU_AE(visit_num, code_num, one_hot_input_dim, input_dim, info_dim, output_dim, output_dim1, output_dim2,output_dim3, voutput_dim, vhidden_dim,hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4,visits_num,codes_num,visit_delt_dates,visitspatientsinfo,patient_num,patient_code,code_delt_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标，使用交叉熵/ SME\n",
    "loss = mtgruae.get_cross_loss()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 多次走iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess= tf.Session() \n",
    "sess.run(init)\n",
    "Loss = np.zeros(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.042512\n",
      "Loss: 0.036165\n",
      "Loss: 0.038223\n",
      "Loss: 0.031517\n",
      "Loss: 0.033212\n",
      "Loss: 0.036574\n",
      "Loss: 0.035183\n",
      "Loss: 0.032555\n",
      "Loss: 0.038279\n",
      "Loss: 0.039314\n",
      "Loss: 0.031534\n",
      "Loss: 0.033046\n",
      "Loss: 0.037364\n",
      "Loss: 0.035777\n",
      "Loss: 0.032443\n",
      "Loss: 0.031197\n",
      "Loss: 0.031380\n",
      "Loss: 0.037866\n",
      "Loss: 0.032057\n",
      "Loss: 0.034172\n"
     ]
    }
   ],
   "source": [
    "# 生成batch训练\n",
    "for i in range(iters):\n",
    "    LOSS = 0\n",
    "    for i in range(10): #学习完整个数据库需要的次数,要按照实际\n",
    "        # 生成batch\n",
    "        xindex, info, t = batch_generator(outFile, visit_num, code_num, patient_num,batch_size)\n",
    "        if len(xindex)==0: continue\n",
    "        _, L = sess.run([optimizer, loss], feed_dict={mtgruae.inputindex: xindex, mtgruae.time: t, mtgruae.info:info})\n",
    "        LOSS += L\n",
    "    Loss[i] = LOSS / 10\n",
    "    print('Loss: %f' %(Loss[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取code的向量\n",
    "# Wi最好relu\n",
    "Wi=sess.run(mtgruae.Wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取数据库中所有visit的patient的表达/新的用户的表达\n",
    "visit_vector, patient_vector = sess.run(mtgruae.get_all_visit_vector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9866611e-01, 9.9999934e-01, 1.7689591e-02, 3.2384160e-01,\n",
       "        9.9999988e-01, 5.5573728e-05, 9.5419500e-07, 2.2339211e-06,\n",
       "        4.2910255e-03, 2.0026885e-08, 7.4299240e-01, 9.9999291e-01,\n",
       "        6.9143327e-09, 3.8227860e-08, 6.3482730e-10, 9.9961030e-01,\n",
       "        9.9999750e-01, 3.2532445e-01, 9.9999934e-01, 6.8571548e-10,\n",
       "        6.4419675e-01, 9.9998844e-01, 3.6672046e-04, 5.8583860e-10,\n",
       "        9.9999875e-01, 3.1186223e-02, 7.0244282e-01, 9.9999821e-01,\n",
       "        8.4483025e-11, 1.3777556e-07, 9.3763759e-03, 5.3719965e-08,\n",
       "        9.9999762e-01, 5.8757067e-02, 1.0026448e-01, 9.9491370e-01,\n",
       "        9.9999982e-01, 9.9999869e-01, 9.9999529e-01, 5.2264112e-04,\n",
       "        1.6715336e-03, 4.5981199e-02, 2.5390431e-02, 6.2860539e-03,\n",
       "        5.1270626e-02, 9.9999732e-01, 8.7920511e-03, 8.9708613e-03,\n",
       "        6.9949059e-03, 1.8713091e-02, 1.5841199e-08, 9.9738002e-01,\n",
       "        5.3471480e-03, 5.0482871e-03, 3.7583418e-02, 9.8355345e-02,\n",
       "        5.8662355e-02, 1.9999946e-02, 3.8527813e-02, 9.7115773e-01,\n",
       "        3.0456390e-02, 4.1778721e-02, 9.9999988e-01, 8.6102784e-01,\n",
       "        3.7516949e-03, 6.3297637e-05, 9.9999875e-01, 1.5548958e-03,\n",
       "        9.9999893e-01, 5.8434182e-09, 9.9993676e-01, 1.5817983e-02,\n",
       "        9.9787211e-01, 9.9999684e-01, 2.1999836e-02, 3.6845654e-02,\n",
       "        9.9988127e-01, 7.7052908e-07, 2.0008883e-11, 9.9915338e-01,\n",
       "        6.0485881e-02, 8.4420711e-01, 9.9999517e-01, 3.0963838e-02,\n",
       "        9.9999845e-01, 1.1177820e-03, 3.6384562e-09, 3.4459442e-02,\n",
       "        1.9592047e-02, 1.4093846e-02, 9.9999678e-01, 4.0384661e-02,\n",
       "        1.2376572e-03, 7.9934150e-01, 1.0185219e-02, 7.0900464e-01,\n",
       "        9.9949598e-01, 9.9972343e-01, 9.9781436e-01, 4.5556836e-02]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_vector[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MODEL/model.ckpt'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, checkpoint_dir + '/model.ckpt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存code visit patient向量\n",
    "pickle.dump(Wi, open(result_dir + '/code_vector' + '.seqs', 'wb'), -1) \n",
    "pickle.dump(visit_vector, open(result_dir + '/visit_vector' + '.seqs', 'wb'), -1) \n",
    "pickle.dump(patient_vector, open(result_dir + '/patient_vector' + '.seqs', 'wb'), -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.212439\n",
      "Loss: 4.750262\n",
      "Loss: 4.520190\n",
      "Loss: 4.613481\n",
      "Loss: 4.385721\n",
      "Loss: 4.476635\n",
      "Loss: 4.364641\n",
      "Loss: 4.281379\n",
      "Loss: 4.243845\n",
      "Loss: 4.033826\n",
      "Loss: 4.233166\n",
      "Loss: 4.160229\n",
      "Loss: 4.152536\n",
      "Loss: 3.949671\n",
      "Loss: 3.995037\n",
      "Loss: 4.049084\n",
      "Loss: 4.197363\n",
      "Loss: 3.865101\n",
      "Loss: 4.103941\n",
      "Loss: 3.926943\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    Loss = np.zeros(iters)\n",
    "    for i in range(iters):\n",
    "        Ll = 0\n",
    "        for i in range(10): #学习完整个数据库需要的次数,要按照实际\n",
    "            # 生成batch,\n",
    "            xindex, info, t = batch_generator('SEQ', visit_num, code_num, patient_num,batch_size)\n",
    "            if len(xindex)==0: continue\n",
    "            _, L = sess.run([optimizer, loss], feed_dict={mtgruae.inputindex: xindex, mtgruae.time: t, mtgruae.info:info})\n",
    "            Ll += L\n",
    "        Loss[i] = Ll / 10\n",
    "        print('Loss: %f' %(Loss[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
