{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import cPickle as pickle\n",
    "import h5py\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MTGRU_predeath2(object):\n",
    "\n",
    "    def init_weights(self, input_dim, output_dim, name=None, std=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=std / math.sqrt(input_dim)), name=name)\n",
    "\n",
    "    def init_bias(self, output_dim, name=None):\n",
    "        return tf.Variable(tf.zeros([output_dim]), name=name)\n",
    "\n",
    "    def __init__(self, visit_num, visit_length, one_hot_input_dim, input_dim, info_dim, output_dim1,\n",
    "                 output_dim2, voutput_dim, output_dim, vhidden_dim, hidden_dim1, hidden_dim2, patient_visit_num,\n",
    "                 patient_visit_length, visit_time, visit_info, patient_num, code_inputindex, code_time):\n",
    "\n",
    "        # 参数patient_visit_num, patient_visit_length, visit_time, visit_info, patient_num, code_inputindex, code_time用来对新的 patient预测疾病\n",
    "        # 一个病人visit的个数\n",
    "        self.visit_num = visit_num\n",
    "        # 一个visit中的code的个数\n",
    "        self.visit_length = visit_length\n",
    "\n",
    "        # 输入的one-hot的维度\n",
    "        self.one_hot_input_dim = one_hot_input_dim\n",
    "        # one-hot变换后作为网络输入的维度\n",
    "        self.input_dim = input_dim\n",
    "        # 患者信息\n",
    "        self.info_dim = info_dim\n",
    "\n",
    "        # 中间层输出,每层gru有一个输出，visit层有一个输出\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "        self.voutput_dim = voutput_dim\n",
    "        # 中间隐藏层，四个gru层，一个visit层\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.vhidden_dim = vhidden_dim\n",
    "\n",
    "        # 最后一层gru最终输出\n",
    "        self.output_dim = output_dim\n",
    "        final_output_dim = 2\n",
    "        self.final_output_dim = 2\n",
    "\n",
    "        # one-hot后的矩阵参数\n",
    "        self.Wi = self.init_weights(one_hot_input_dim, input_dim, name='OneHot_w')\n",
    "        self.bi = self.init_bias(input_dim, name='OneHot_w')\n",
    "\n",
    "        # encoder的GRU参数\n",
    "        self.Wz_enc = self.init_weights(input_dim, hidden_dim1, name='Update_wx_enc')\n",
    "        self.Uz_enc = self.init_weights(hidden_dim1, hidden_dim1, name='Update_wh_enc')\n",
    "        self.bz_enc = self.init_bias(hidden_dim1, name='Update_bias_enc')\n",
    "\n",
    "        self.Wz_enc2 = self.init_weights(output_dim1, hidden_dim2, name='Update_wx_enc2')\n",
    "        self.Uz_enc2 = self.init_weights(hidden_dim2, hidden_dim2, name='Update_wh_enc2')\n",
    "        self.bz_enc2 = self.init_bias(hidden_dim2, name='Update_bias_enc2')\n",
    "\n",
    "        self.Wr_enc = self.init_weights(input_dim, hidden_dim1, name='Reset_wx_enc')\n",
    "        self.Ur_enc = self.init_weights(hidden_dim1, hidden_dim1, name='Reset_wh_enc')\n",
    "        self.br_enc = self.init_bias(hidden_dim1, name='Reset_bias_enc')\n",
    "\n",
    "        self.Wr_enc2 = self.init_weights(output_dim1, hidden_dim2, name='Reset_wx_enc2')\n",
    "        self.Ur_enc2 = self.init_weights(hidden_dim2, hidden_dim2, name='Reset_wh_enc2')\n",
    "        self.br_enc2 = self.init_bias(hidden_dim2, name='Reset_bias_enc2')\n",
    "\n",
    "        self.Wd_enc = tf.ones([1, self.hidden_dim1], dtype=tf.float32, name='Decay_w_enc')\n",
    "        self.Wd_enc2 = tf.ones([1, self.hidden_dim2], dtype=tf.float32, name='Decay_w_enc2')\n",
    "\n",
    "        self.Wh_enc = self.init_weights(self.input_dim, self.hidden_dim1, name='Canditateh_wx_enc')\n",
    "        self.Uh_enc = self.init_weights(self.hidden_dim1, self.hidden_dim1, name='Canditateh_wh_enc')\n",
    "        self.bh_enc = self.init_bias(self.hidden_dim1, name='Canditateh_bias_enc')\n",
    "\n",
    "        self.Wh_enc2 = self.init_weights(self.output_dim1, self.hidden_dim2, name='Canditateh_wx_enc2')\n",
    "        self.Uh_enc2 = self.init_weights(self.hidden_dim2, self.hidden_dim2, name='Canditateh_wh_enc2')\n",
    "        self.bh_enc2 = self.init_bias(self.hidden_dim2, name='Canditateh_bias_enc2')\n",
    "\n",
    "        # visit层的参数\n",
    "        # 输入是上一层的输出加上病人信息\n",
    "        self.Wz_v = self.init_weights(output_dim2 + info_dim, vhidden_dim, name='Update_wx_v')\n",
    "        self.Uz_v = self.init_weights(vhidden_dim, vhidden_dim, name='Update_wh_v')\n",
    "        self.bz_v = self.init_bias(vhidden_dim, name='Update_bias_dec')\n",
    "\n",
    "        self.Wr_v = self.init_weights(output_dim2 + info_dim, vhidden_dim, name='Reset_wx_v')\n",
    "        self.Ur_v = self.init_weights(vhidden_dim, vhidden_dim, name='Reset_wh_v')\n",
    "        self.br_v = self.init_bias(vhidden_dim, name='Reset_bias_v')\n",
    "\n",
    "        self.Wd_v = tf.ones([1, self.vhidden_dim], dtype=tf.float32, name='Decay_w_v')\n",
    "\n",
    "        self.Wh_v = self.init_weights(self.output_dim2 + info_dim, self.vhidden_dim, name='Canditateh_wx_v')\n",
    "        self.Uh_v = self.init_weights(self.vhidden_dim, self.vhidden_dim, name='Canditateh_wh_v')\n",
    "        self.bh_v = self.init_bias(self.vhidden_dim, name='Canditateh_bias_v')\n",
    "\n",
    "        # 输出层\n",
    "        # encoder1\n",
    "        self.Wo1 = self.init_weights(hidden_dim1, output_dim1, name='Output1_weight')\n",
    "        self.bo1 = self.init_bias(output_dim1, name='Output1_bias')\n",
    "        # encoder2\n",
    "        self.Wo2 = self.init_weights(hidden_dim2, output_dim2, name='Output2_weight')\n",
    "        self.bo2 = self.init_bias(output_dim2, name='Output2_bias')\n",
    "        # visit层的输出\n",
    "        self.Wo = self.init_weights(vhidden_dim, output_dim, name='Output_w')\n",
    "        self.bo = self.init_bias(output_dim, name='Output_bias')\n",
    "        # 最终输出全连接\n",
    "        self.Wf = self.init_weights(output_dim, final_output_dim, name='Final_output_w')\n",
    "        self.bf = self.init_bias(final_output_dim, name='Final_output_bias')\n",
    "\n",
    "        # 输入占位符\n",
    "        # [batch size x seq length x input dim]\n",
    "        self.inputindex = tf.placeholder(dtype=tf.int32, shape=[None, None])\n",
    "        self.one_hot_input = tf.one_hot(indices=self.inputindex, depth=self.one_hot_input_dim, axis=2)  # 输入\n",
    "        self.labels = tf.placeholder('float', shape=[None, final_output_dim])  # 标签\n",
    "        self.time = tf.placeholder('float', [None, None])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.info = tf.placeholder('float', shape=[None, None, self.info_dim])  # 患者人口信息\n",
    "\n",
    "        # 对新的人来预测疾病\n",
    "        self.patient_visit_num = patient_visit_num  # 一个患者的visit个数\n",
    "        self.patient_visit_length = patient_visit_length  # 一个患者的visit中的code个数\n",
    "        self.visit_time = visit_time\n",
    "        self.visit_info = visit_info\n",
    "        self.patient_num = patient_num\n",
    "        self.code_inputindex = code_inputindex  # 一个病人所有code的index\n",
    "        self.code_time = code_time\n",
    "\n",
    "    # 对输入的one-hot使用矩阵处理，code的嵌入向量1\n",
    "    def get_input(self, one_hot_input):\n",
    "        input = tf.matmul(one_hot_input, self.Wi) + self.bi\n",
    "        return input\n",
    "\n",
    "    # encoder第一层TGRU cell，code的嵌入向量2\n",
    "    def TGRU_encoder_cell1(self, prev_h, concat_input):\n",
    "\n",
    "        # concat_input:[batch_size x input_dim+1]\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "        x = tf.slice(concat_input, [0, 1], [batch_size, self.input_dim])\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "\n",
    "        ft = self.map_elapse_time(t, self.hidden_dim1)\n",
    "\n",
    "        z = tf.sigmoid(tf.matmul(x, self.Wz_enc) + tf.matmul(prev_h, self.Uz_enc) + self.bz_enc)\n",
    "        r = tf.sigmoid(tf.matmul(x, self.Wr_enc) + tf.matmul(prev_h, self.Ur_enc) + self.br_enc)\n",
    "        d = tf.matmul(ft, self.Wd_enc)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(x, self.Wh_enc) + tf.matmul(r * h_, self.Uh_enc) + self.bh_enc)\n",
    "\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    # encoder第二层TGRU cell\n",
    "    def TGRU_encoder_cell2(self, prev_h, concat_input):\n",
    "\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "        x = tf.slice(concat_input, [0, 1], [batch_size, self.output_dim1])\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "\n",
    "        ft = self.map_elapse_time(t, self.hidden_dim2)\n",
    "\n",
    "        z = tf.sigmoid(tf.matmul(x, self.Wz_enc2) + tf.matmul(prev_h, self.Uz_enc2) + self.bz_enc2)\n",
    "        r = tf.sigmoid(tf.matmul(x, self.Wr_enc2) + tf.matmul(prev_h, self.Ur_enc2) + self.br_enc2)\n",
    "        d = tf.matmul(ft, self.Wd_enc2)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(x, self.Wh_enc2) + tf.matmul(r * h_, self.Uh_enc2) + self.bh_enc2)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    # visit层的TGURU cell\n",
    "    # 此时的输入与其他cell的不同，concat_input应该是多个时刻的code的，而其他的输入都是一个时刻的code，因此在后面对get_encoder_h2输出的结果做处理\n",
    "    # 此时的concat_input是一个三维向量，也就是有多时刻的code，每个时刻有一个时间和batch个数个code\n",
    "    # [visit_length x batch_size x 1+input_dim ]\n",
    "    # 创建每个visit的长度的参数，来方便创建多个输入x\n",
    "    def TGRU_visit_cell(self, prev_h, concat_input):\n",
    "\n",
    "        # visit_length x batch_size x input_dim+info_dim+info_dim+1\n",
    "        batch_size = tf.shape(concat_input)[1]\n",
    "\n",
    "        # 只获取第一个时间\n",
    "        t = tf.slice(concat_input, [0, 0, 0], [1, batch_size, 1])\n",
    "        # 只获取第一个info\n",
    "        info = tf.slice(concat_input, [0, 0, 1], [1, batch_size, self.info_dim])\n",
    "\n",
    "        # 循环创建局部变量\n",
    "        for i in range(self.visit_length):\n",
    "            locals()['x' + str(i)] = tf.slice(concat_input, [i, 0, 1 + self.info_dim],\n",
    "                                              [1, batch_size, self.output_dim2])\n",
    "\n",
    "        # 获得visit层的输入\n",
    "        visit_x = tf.zeros([batch_size, self.output_dim2], dtype=tf.float32)\n",
    "        for i in range(self.visit_length):\n",
    "            visit_x += locals()['x' + str(i)]\n",
    "\n",
    "        visit_x = visit_x / self.visit_length\n",
    "\n",
    "        # 把输入和病人信息放在一个向量中\n",
    "        visit_x = tf.concat([visit_x, info], 2)\n",
    "        # 3维变2维 第一维是大小是1，没用\n",
    "        visit_x = tf.reshape(visit_x, [tf.shape(visit_x)[1], tf.shape(visit_x)[2]])\n",
    "        t = tf.reshape(t, [tf.shape(t)[1], tf.shape(t)[2]])\n",
    "\n",
    "        # visit嵌入向量经过GRU输出\n",
    "        ft = self.map_elapse_time(t, self.vhidden_dim)\n",
    "        z = tf.sigmoid(tf.matmul(visit_x, self.Wz_v) + tf.matmul(prev_h, self.Uz_v) + self.bz_v)\n",
    "        r = tf.sigmoid(tf.matmul(visit_x, self.Wr_v) + tf.matmul(prev_h, self.Ur_v) + self.br_v)\n",
    "        d = tf.matmul(ft, self.Wd_v)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(visit_x, self.Wh_v) + tf.matmul(r * h_, self.Uh_v) + self.bh_v)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    # 每个输出层的操作\n",
    "    # encoder1的输出\n",
    "    def get_output1(self, h):\n",
    "        output = tf.matmul(h, self.Wo1) + self.bo1\n",
    "        # output = tf.nn.softmax(tf.nn.relu(tf.matmul(state, self.Wo) + self.bo))\n",
    "        return output\n",
    "\n",
    "    # encoder2的输出\n",
    "    def get_output2(self, h):\n",
    "        output = tf.matmul(h, self.Wo2) + self.bo2\n",
    "        # output = tf.nn.softmax(tf.nn.relu(tf.matmul(state, self.Wo) + self.bo))\n",
    "        return output\n",
    "\n",
    "    # 输出层，对h进行全连接输出\n",
    "    def get_output(self, h):\n",
    "        output = tf.nn.relu(tf.matmul(h, self.Wo) + self.bo)\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "    # encoder1的输出h\n",
    "    def get_encoder1_h(self):  # Returns all hidden states for the samples in a batch\n",
    "\n",
    "        convert_input = self.get_input(self.one_hot_input)\n",
    "        batch_size = tf.shape(convert_input)[0]\n",
    "        scan_input = tf.transpose(convert_input, perm=[1, 0, 2])  # scan input is [seq_length x batch_size x input_dim]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim1], tf.float32)\n",
    "\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        concat_input = tf.concat([scan_time, scan_input], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder1_h = tf.scan(self.TGRU_encoder_cell1, concat_input, initializer=initial_hidden, name='encoder1_h')\n",
    "\n",
    "        return encoder1_h\n",
    "\n",
    "    # encoder2的输出h\n",
    "    def get_encoder2_h(self):  # Returns all hidden states for the samples in a batch\n",
    "\n",
    "        encoder1_h = self.get_encoder1_h()\n",
    "        encoder1_output = tf.map_fn(self.get_output1, encoder1_h)\n",
    "\n",
    "        batch_size = tf.shape(encoder1_h)[1]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim2], tf.float32)\n",
    "\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        concat_input = tf.concat([scan_time, encoder1_output], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder2_h = tf.scan(self.TGRU_encoder_cell2, concat_input, initializer=initial_hidden, name='encoder2_h')\n",
    "        return encoder2_h\n",
    "\n",
    "    # visit最后的输出h\n",
    "    def get_visit_h(self):  # Returns all hidden states for the samples in a batch\n",
    "\n",
    "        encoder2_h = self.get_encoder2_h()\n",
    "        encoder2_output = tf.map_fn(self.get_output2, encoder2_h)\n",
    "\n",
    "        batch_size = tf.shape(encoder2_h)[1]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "\n",
    "        # make info [batch_size x seq_length x info_dim] --> [seq_length x batch_size x info_dim]\n",
    "        scan_info = tf.transpose(self.info, [1, 0, 2])\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "\n",
    "        # [seq_length x batch_size x input_dim+info_dim+1]\n",
    "        concat_input = tf.concat([scan_time, scan_info, encoder2_output], 2)\n",
    "\n",
    "        # 已经知道了visit的个数和长度，可以直接把concat_input拆分成visit个\n",
    "        # [visit_num x visit_length x batch_size x input_dim+info_dim_1]\n",
    "        initial_hidden = tf.zeros([batch_size, self.vhidden_dim], tf.float32)\n",
    "        concat_input = tf.reshape(concat_input, [self.visit_num, self.visit_length, tf.shape(concat_input)[1],\n",
    "                                                 tf.shape(concat_input)[2]])\n",
    "        visit_h = tf.scan(self.TGRU_visit_cell, concat_input, initializer=initial_hidden, name='visit')\n",
    "\n",
    "        return visit_h\n",
    "\n",
    "    def get_cross_loss(self):\n",
    "        output = self.get_visit_h()\n",
    "        batch_size = tf.shape(output)[1]\n",
    "        final_output = tf.map_fn(self.get_output, output)\n",
    "        fo = tf.zeros([batch_size, self.output_dim], dtype=tf.float32)\n",
    "        for i in range(self.visit_num ):\n",
    "            fo=fo+final_output[i]\n",
    "        fo=fo/self.visit_num\n",
    "\n",
    "        fo = tf.nn.dropout(fo, self.keep_prob)\n",
    "        pre_output = tf.matmul(fo, self.Wf) + self.bf\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=pre_output))\n",
    "\n",
    "        y_pred = tf.argmax(pre_output, 1)\n",
    "        y = tf.argmax(self.labels, 1)\n",
    "\n",
    "        return cross_entropy, y_pred, y, pre_output, self.labels\n",
    "\n",
    "    def map_elapse_time(self, t, dim):\n",
    "        c1 = tf.constant(1, dtype=tf.float32)\n",
    "        c2 = tf.constant(2.7183, dtype=tf.float32)\n",
    "\n",
    "        T = tf.div(c1, tf.log(t + c2), name='Log_elapse_time')\n",
    "        # T = tf.div(c1, tf.add(t , c1), name='Log_elapse_time')\n",
    "        return T\n",
    "\n",
    "    # 获取数据集中病人的预测\n",
    "    def TGRU_visit_cell2(self, prev_h, concat_input):\n",
    "        # batch_size x input_dim+info_dim+info_dim+1\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "\n",
    "        # 时间\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "        # visit_info\n",
    "        info = tf.slice(concat_input, [0, 1], [batch_size, self.info_dim])\n",
    "        # 输入\n",
    "        x = tf.slice(concat_input, [0, 1 + self.info_dim], [batch_size, self.output_dim2])\n",
    "\n",
    "        # 把输入和病人信息放在一个向量中\n",
    "        visit_x = tf.concat([x, info], 1)\n",
    "\n",
    "        # visit嵌入向量经过GRU输出\n",
    "        ft = self.map_elapse_time(t, self.vhidden_dim)\n",
    "        z = tf.sigmoid(tf.matmul(visit_x, self.Wz_v) + tf.matmul(prev_h, self.Uz_v) + self.bz_v)\n",
    "        r = tf.sigmoid(tf.matmul(visit_x, self.Wr_v) + tf.matmul(prev_h, self.Ur_v) + self.br_v)\n",
    "        d = tf.matmul(ft, self.Wd_v)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(visit_x, self.Wh_v) + tf.matmul(r * h_, self.Uh_v) + self.bh_v)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    # 备注的矩阵大小是假设一个用户的code有39个，2个visit，总共的code种类259个\n",
    "    def get_encoder1_h_forPre(self, no):  # Returns all hidden states for the samples in a batch\n",
    "        convert_input = self.get_input(\n",
    "            tf.one_hot(indices=self.code_inputindex[no], depth=self.one_hot_input_dim, axis=1))\n",
    "        batch_size = 1\n",
    "\n",
    "        # (39, 1, 259)\n",
    "        scan_input = tf.transpose([convert_input],\n",
    "                                  perm=[1, 0, 2])  # scan input is [seq_length x batch_size x input_dim]\n",
    "        # (39, 1)\n",
    "        scan_time = tf.transpose([tf.to_float(self.code_time[no])])  # scan_time [seq_length x batch_size]\n",
    "        # (1, 150)\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim1], tf.float32)\n",
    "        # (39, 1, 1) make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        # (39, 1, 260)\n",
    "        concat_input = tf.concat([scan_time, scan_input], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder1_h = tf.scan(self.TGRU_encoder_cell1, concat_input, initializer=initial_hidden,\n",
    "                             name='encoder1_h_forVector')\n",
    "        return encoder1_h\n",
    "\n",
    "    def get_encoder2_h_forPre(self, no):  # Returns all hidden states for the samples in a batch\n",
    "        # (39, 1, 150)\n",
    "        encoder1_h = self.get_encoder1_h_forPre(no)\n",
    "        # (39, 1, 150)\n",
    "        encoder1_output = tf.map_fn(self.get_output1, encoder1_h)\n",
    "        batch_size = 1\n",
    "        # (39, 1)\n",
    "        scan_time = tf.transpose([tf.to_float(self.code_time[no])])  # scan_time [seq_length x batch_size]\n",
    "        # (1, 150)\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim2], tf.float32)\n",
    "        # (39, 1, 1) # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        # (39, 1, 151)\n",
    "        concat_input = tf.concat([scan_time, encoder1_output], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder2_h = tf.scan(self.TGRU_encoder_cell2, concat_input, initializer=initial_hidden,\n",
    "                             name='encoder2_h_forVector')\n",
    "        return encoder2_h\n",
    "\n",
    "    def get_one_visit_Pre(self, no):\n",
    "        encoder2_h = self.get_encoder2_h_forPre(no)\n",
    "        encoder2_output = tf.map_fn(self.get_output2, encoder2_h)\n",
    "        batch_size = 1\n",
    "        n = 0\n",
    "        x = []\n",
    "        for i in range(self.patient_visit_num[no][0]):\n",
    "            # (1, 150)\n",
    "            visit_input = tf.zeros([batch_size, self.output_dim2], dtype=tf.float32)\n",
    "            for j in range(self.patient_visit_length[no][i]):\n",
    "                visit_input += encoder2_output[n]\n",
    "                n = n + 1\n",
    "            x.append(visit_input / self.patient_visit_length[no][i])\n",
    "        # (2, 1)\n",
    "        scan_time = tf.transpose([tf.to_float(self.visit_time[no])])  # scan_time [seq_length x batch_size]\n",
    "        # (2, 1, 2) make info [batch_size x seq_length x info_dim] --> [seq_length x batch_size x info_dim]\n",
    "        scan_info = tf.transpose(tf.to_float([self.visit_info[no]]), [1, 0, 2])\n",
    "        # (2, 1, 1)make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        # (2, 1, 153)[seq_length x batch_size x input_dim+info_dim+1]\n",
    "        concat_input = tf.concat([scan_time, scan_info, x], 2)\n",
    "        # (1,100)\n",
    "        initial_hidden = tf.zeros([batch_size, self.vhidden_dim], tf.float32)\n",
    "\n",
    "        visit_h = tf.scan(self.TGRU_visit_cell2, concat_input, initializer=initial_hidden, name='visit')\n",
    "        return visit_h\n",
    "\n",
    "\n",
    "    def get_one_output_pre(self, h):\n",
    "        output = tf.nn.relu(tf.matmul(h, self.Wo) + self.bo)\n",
    "        return output\n",
    "\n",
    "    def get_all_Pre_sigmoid(self):\n",
    "        pres = []\n",
    "        for no in range(self.patient_num):\n",
    "            h = self.get_one_visit_Pre(no)\n",
    "            batch_size = tf.shape(h)[1]\n",
    "            h_output = tf.map_fn(self.get_one_output_pre, h)\n",
    "            final_output = tf.zeros([batch_size, self.output_dim], dtype=tf.float32)\n",
    "            for i in range(self.patient_visit_num[no][0]):\n",
    "                final_output = final_output + h_output[i]\n",
    "            final_output = final_output / self.patient_visit_num[no][0]\n",
    "\n",
    "            pre = tf.matmul(final_output, self.Wf) + self.bf\n",
    "            pre = tf.sigmoid(pre)\n",
    "            pres.append(pre)\n",
    "        return pres\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch生成器\n",
    "# bacth中有cutting和padding，由于长度不同，因此开始学习的时候可以每个数据单独学习\n",
    "# 生成的batch里的病人visit和code的个数一样\n",
    "def batch_generator(outFile, visit_num, code_num, patient_num, batch_size):\n",
    "    patient_code_file=open(outFile + '/patient_code' + '.seqs','rb')\n",
    "    patient_code=pickle.load(patient_code_file)\n",
    "    \n",
    "    codespatientsinfo_file=open(outFile + '/codespatientsinfo' + '.seqs','rb')\n",
    "    codespatientsinfo=pickle.load(codespatientsinfo_file)\n",
    "    \n",
    "    visit_delt_dates_file=open(outFile + '/visit_delt_dates' + '.seqs','rb')\n",
    "    visit_delt_dates=pickle.load(visit_delt_dates_file)\n",
    "    \n",
    "    code_delt_dates_file=open(outFile + '/code_delt_dates' + '.seqs','rb')\n",
    "    code_delt_dates=pickle.load(code_delt_dates_file)\n",
    "    \n",
    "    visits_num_file=open(outFile + '/visits_num' + '.seqs','rb')\n",
    "    visits_num=pickle.load(visits_num_file)\n",
    "    \n",
    "    codes_num_file=open(outFile + '/codes_num' + '.seqs','rb')\n",
    "    codes_num=pickle.load(codes_num_file)\n",
    "    \n",
    "    death_labels_file=open(outFile + '/death_labels' + '.seqs','rb')\n",
    "    death_labels=pickle.load(death_labels_file)\n",
    "    \n",
    "    patient_code_file.close()\n",
    "    codespatientsinfo_file.close()\n",
    "    visit_delt_dates_file.close()\n",
    "    code_delt_dates_file.close()\n",
    "    visits_num_file.close()\n",
    "    codes_num_file.close()\n",
    "\n",
    "    batch_patient_code=[]\n",
    "    batch_code_delt_dates=[]\n",
    "    batch_codespatientsinfo=[]\n",
    "    batch_visit_delt_dates=[]\n",
    "    batch_labels=[]\n",
    "\n",
    "    \n",
    "    # padding and cutting\n",
    "    for i in range(batch_size):\n",
    "        j=random.randint(0, patient_num-1)\n",
    "        if visits_num[j][0]== visit_num:\n",
    "           # print 'pick',j\n",
    "            code=[]\n",
    "            code_date=[]\n",
    "            code_info=[]\n",
    "            for k in range(visit_num):\n",
    "                if  codes_num[j][k]> code_num:\n",
    "                    if k==0: \n",
    "                        code.extend(patient_code[j][0:code_num])\n",
    "                        code_date.extend(code_delt_dates[j][0:code_num])\n",
    "                        code_info.extend(codespatientsinfo[j][0:code_num])\n",
    "                    else:\n",
    "                        start=k*codes_num[j][k-1]\n",
    "                        code.extend(patient_code[j][start :start+code_num])\n",
    "                        code_date.extend(code_delt_dates[j][start:start+code_num])\n",
    "                        code_info.extend(codespatientsinfo[j][start:start+code_num])\n",
    "                elif codes_num[j][k] < code_num:\n",
    "                    if k==0:\n",
    "                        code.extend(patient_code[j][:codes_num[j][k]])\n",
    "                        code.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_date.extend(code_delt_dates[j][:codes_num[j][k]])\n",
    "                        code_date.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_info.extend(codespatientsinfo[j][:codes_num[j][k]])\n",
    "                        code_info.extend([[0,0]]*(code_num-codes_num[j][k]))\n",
    "                    else:\n",
    "                        start2=0\n",
    "                        for n in range(k):\n",
    "                            start2+=codes_num[j][n]\n",
    "                        code.extend(patient_code[j][start2: start2+codes_num[j][k]])\n",
    "                        code.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_date.extend(code_delt_dates[j][start2:start2+codes_num[j][k]])\n",
    "                        code_date.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_info.extend(codespatientsinfo[j][start2:start2+codes_num[j][k]])\n",
    "                        code_info.extend([[0,0]]*(code_num-codes_num[j][k]))\n",
    "                else:\n",
    "                    code.extend(patient_code[j])\n",
    "                    code_date.extend(code_delt_dates[j])\n",
    "                    code_info.extend(codespatientsinfo[j])\n",
    "            batch_patient_code.append(code)\n",
    "            batch_code_delt_dates.append(code_date)  \n",
    "            batch_codespatientsinfo.append(code_info)\n",
    "            batch_labels.append(death_labels[j])\n",
    "                    \n",
    "        \n",
    "        # 考虑随机生成的时候可能有的数据始终选不到，因此还要有一个顺序生成\n",
    "    return batch_patient_code, batch_codespatientsinfo, batch_code_delt_dates,batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "learning_rate = 1e-3\n",
    "iters = 20\n",
    "dropout=1.0\n",
    "\n",
    "# 网络参数\n",
    "info_dim=2\n",
    "icd9_num=259\n",
    "one_hot_input_dim=icd9_num # 数据len(types)=259, one-hot的长度应该是259\n",
    "input_dim = 200 \n",
    "hidden_dim1 =150\n",
    "output_dim1=150\n",
    "hidden_dim2=150\n",
    "output_dim2=150\n",
    "vhidden_dim=100\n",
    "voutput_dim=150\n",
    "output_dim=200\n",
    "\n",
    "# 生成batch数据参数\n",
    "outFile='SEQ'\n",
    "checkpoint_dir_pre='MODEL_pre'\n",
    "result_dir='RESULT'\n",
    "batch_size=4\n",
    "visit_num=2 #visit_num=random.randint() \n",
    "code_num=10 #code_num=random.randint() # code_num=visit_length\n",
    "patient_num=14 #病人的总个数\n",
    "\n",
    "\n",
    "\n",
    "# 数据集获取\n",
    "patient_code_file=open(outFile + '/patient_code' + '.seqs','rb')\n",
    "patient_code=pickle.load(patient_code_file)\n",
    "\n",
    "code_delt_dates_file=open(outFile + '/code_delt_dates' + '.seqs','rb')\n",
    "code_delt_dates=pickle.load(code_delt_dates_file)\n",
    "\n",
    "visit_delt_dates_file=open(outFile + '/visit_delt_dates' + '.seqs','rb')\n",
    "visit_delt_dates=pickle.load(visit_delt_dates_file)\n",
    "\n",
    "visits_num_file=open(outFile + '/visits_num' + '.seqs','rb')\n",
    "visits_num=pickle.load(visits_num_file)\n",
    "\n",
    "codes_num_file=open(outFile + '/codes_num' + '.seqs','rb')\n",
    "codes_num=pickle.load(codes_num_file)\n",
    "\n",
    "visitspatientsinfo_file=open(outFile + '/visitspatientsinfo' + '.seqs','rb')\n",
    "visitspatientsinfo=pickle.load(visitspatientsinfo_file)\n",
    "\n",
    "codespatientsinfo_file=open(outFile + '/codespatientsinfo' + '.seqs','rb')\n",
    "codespatientsinfo=pickle.load(codespatientsinfo_file)\n",
    "\n",
    "death_labels_file=open(outFile + '/death_labels' + '.seqs','rb')\n",
    "death_labels=pickle.load(death_labels_file)\n",
    "\n",
    "\n",
    "patient_code_file.close()\n",
    "code_delt_dates_file.close()\n",
    "visit_delt_dates_file.close()\n",
    "visits_num_file.close()\n",
    "codes_num_file.close()\n",
    "visitspatientsinfo_file.close()\n",
    "codespatientsinfo_file.close()\n",
    "death_labels_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化网络\n",
    "mtgrupredeath2 = MTGRU_predeath2(visit_num, code_num, one_hot_input_dim, input_dim, info_dim, output_dim1,\n",
    "                 output_dim2, voutput_dim,output_dim, vhidden_dim, hidden_dim1, hidden_dim2,visits_num,codes_num,visit_delt_dates,visitspatientsinfo,patient_num,patient_code,code_delt_dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标，使用交叉熵/ SME\n",
    "cross_entropy, y_pred, y, logits, labels = mtgrupredeath2.get_cross_loss()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "init = tf.global_variables_initializer()\n",
    "sess= tf.Session() \n",
    "sess.run(init)\n",
    "Loss = np.zeros(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss, Accuracy, AUC, AUC Macro: 0.000962 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000446 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000456 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000474 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000394 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000279 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000232 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000197 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000170 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000269 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000194 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000262 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000236 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000149 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000115 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000117 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000094 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000117 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000101 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000142 1.000000 1.000000 1.000000\n"
     ]
    }
   ],
   "source": [
    " # 生成batch训练\n",
    "for i in range(iters):\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    Labels = []\n",
    "    Logits = []\n",
    "    LOSS = 0 \n",
    "    \n",
    "    n=0\n",
    "    for i in range(10): #学习完整个数据库需要的次数,要按照实际\n",
    "        \n",
    "        # 生成batch\n",
    "        xindex, info, t,batch_labels = batch_generator(outFile, visit_num, code_num, patient_num,batch_size)\n",
    "        if len(xindex)==0: continue\n",
    "        _, c_train, y_pred_train, y_train, logits_train, labels_train = sess.run([optimizer, cross_entropy, y_pred, y, logits, labels ], feed_dict={mtgrupredeath2.inputindex: xindex, mtgrupredeath2.time: t, mtgrupredeath2.info:info,mtgrupredeath2.labels:batch_labels,mtgrupredeath2.keep_prob:dropout})\n",
    "        LOSS += c_train\n",
    "        \n",
    "        if n > 0:\n",
    "            Y_true = np.concatenate([Y_true, y_train], 0)\n",
    "            Y_pred = np.concatenate([Y_pred, y_pred_train], 0)\n",
    "            Labels = np.concatenate([Labels, labels_train], 0)\n",
    "            Logits = np.concatenate([Logits, logits_train], 0)\n",
    "        else:\n",
    "            Y_true = y_train\n",
    "            Y_pred = y_pred_train\n",
    "            Labels = labels_train\n",
    "            Logits = logits_train\n",
    "        n+=1\n",
    "    \n",
    "    Loss[i] = LOSS / 10\n",
    "    \n",
    "    \n",
    "    total_acc = accuracy_score(Y_true, Y_pred)\n",
    "    total_auc = roc_auc_score(Labels, Logits, average='micro')\n",
    "    total_auc_macro = roc_auc_score(Labels, Logits, average='macro')\n",
    "    \n",
    "    print('Loss, Accuracy, AUC, AUC Macro: %f %f %f %f' %(Loss[i],total_acc,total_auc,total_auc_macro))\n",
    "    \n",
    "    #print('Loss %f' %(Loss[i]))\n",
    "    #print(\"Train Accuracy = {:.3f}\".format(total_acc))\n",
    "    #print(\"Train AUC = {:.3f}\".format(total_auc))\n",
    "    #print(\"Train AUC Macro = {:.3f}\".format(total_auc_macro))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-a4fad8d1905e>:353: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "# 在整个数据集上的准确度\n",
    "pres = sess.run(mtgrupredeath2.get_all_Pre_sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, AUC, AUC Macro: 0.857143 0.836735 0.836735\n"
     ]
    }
   ],
   "source": [
    "# 所有数据的准确度\n",
    "test_pres=[]\n",
    "test_death_labels=[]\n",
    "for i in range(len(pres)):\n",
    "    if pres[i][0][0]>pres[i][0][1]: test_pres.append(0)\n",
    "    else: test_pres.append(1)\n",
    "    test_death_labels.append(death_labels[i][1])\n",
    "\n",
    "format_pres=[]\n",
    "for pre in pres:\n",
    "    format_pres.append(pre[0][1])\n",
    "    \n",
    "all_acc = accuracy_score(test_death_labels, test_pres)\n",
    "all_auc = roc_auc_score(test_death_labels, format_pres, average='micro')\n",
    "all_auc_macro = roc_auc_score(test_death_labels, format_pres, average='macro')\n",
    "print('Accuracy, AUC, AUC Macro: %f %f %f' %(all_acc,all_auc,all_auc_macro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
