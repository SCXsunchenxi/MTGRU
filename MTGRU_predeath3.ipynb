{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import cPickle as pickle\n",
    "import h5py\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import cPickle as pickle\n",
    "import h5py\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "\n",
    "class MedTrans(object):\n",
    "    def init_weights(self, input_dim, output_dim, name=None, std=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=std / math.sqrt(input_dim)), name=name)\n",
    "\n",
    "    def init_bias(self, output_dim, name=None):\n",
    "        return tf.Variable(tf.zeros([output_dim]), name=name)\n",
    "\n",
    "    def __init__(self, num_heads, attention_blocks_num, f_dim, visit_num, visit_length, one_hot_input_dim,\n",
    "                 input_dim, info_dim, voutput_dim, output_dim, vhidden_dim):\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_blocks_num = attention_blocks_num\n",
    "        self.f_dim = f_dim\n",
    "\n",
    "        self.visit_num = visit_num\n",
    "        self.visit_length = visit_length\n",
    "\n",
    "        self.one_hot_input_dim = one_hot_input_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.info_dim = info_dim\n",
    "        self.voutput_dim = voutput_dim\n",
    "        self.vhidden_dim = vhidden_dim\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        final_output_dim = 2\n",
    "        self.final_output_dim = 2\n",
    "\n",
    "        # one-hot后的矩阵参数\n",
    "        self.Wi = self.init_weights(one_hot_input_dim, input_dim, name='OneHot_w')\n",
    "        self.bi = self.init_bias(input_dim, name='OneHot_w')\n",
    "\n",
    "        # visit层的参数\n",
    "        # 输入是上一层的输出加上病人信息\n",
    "        self.Wz_v = self.init_weights(input_dim + info_dim, vhidden_dim, name='Update_wx_v')\n",
    "        self.Uz_v = self.init_weights(vhidden_dim, vhidden_dim, name='Update_wh_v')\n",
    "        self.bz_v = self.init_bias(vhidden_dim, name='Update_bias_dec')\n",
    "\n",
    "        self.Wr_v = self.init_weights(input_dim + info_dim, vhidden_dim, name='Reset_wx_v')\n",
    "        self.Ur_v = self.init_weights(vhidden_dim, vhidden_dim, name='Reset_wh_v')\n",
    "        self.br_v = self.init_bias(vhidden_dim, name='Reset_bias_v')\n",
    "        self.Wd_v = tf.ones([1, vhidden_dim], dtype=tf.float32, name='Decay_w_v')\n",
    "        self.Wh_v = self.init_weights(input_dim + info_dim, vhidden_dim, name='Canditateh_wx_v')\n",
    "        self.Uh_v = self.init_weights(vhidden_dim, vhidden_dim, name='Canditateh_wh_v')\n",
    "        self.bh_v = self.init_bias(vhidden_dim, name='Canditateh_bias_v')\n",
    "\n",
    "        # 输出层\n",
    "        # visit\n",
    "        self.Wo = self.init_weights(vhidden_dim, output_dim, name='Output_w')\n",
    "        self.bo = self.init_bias(output_dim, name='Output_bias')\n",
    "        # 最终输出全连接\n",
    "        self.Wf = self.init_weights(output_dim, final_output_dim, name='Final_output_w')\n",
    "        self.bf = self.init_bias(final_output_dim, name='Final_output_bias')\n",
    "\n",
    "        # 输入占位符\n",
    "        # [batch size x seq length x input dim]\n",
    "        self.inputindex = tf.placeholder(dtype=tf.int32, shape=[None, None])\n",
    "        self.one_hot_input = tf.one_hot(indices=self.inputindex, depth=one_hot_input_dim, axis=2)  # 输入\n",
    "        self.labels = tf.placeholder('float', shape=[None, final_output_dim])  # 标签\n",
    "        self.time = tf.placeholder('float', [None, None])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.info = tf.placeholder('float', shape=[None, None, info_dim])  # 患者人口信息\n",
    "\n",
    "\n",
    "\n",
    "    # 时间衰减函数\n",
    "    def map_elapse_time(self, t, dim):\n",
    "        c1 = tf.constant(1, dtype=tf.float32)\n",
    "        c2 = tf.constant(2.7183, dtype=tf.float32)\n",
    "\n",
    "        T = tf.div(c1, tf.log(t + c2), name='Log_elapse_time')\n",
    "        # T = tf.div(c1, tf.add(t , c1), name='Log_elapse_time')\n",
    "        return T\n",
    "\n",
    "        # layer normalization\n",
    "\n",
    "    def ln(self, inputs, epsilon=1e-8, scope=\"ln\"):\n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "            # tf.nn.moments计算均值和方差，[-1]是最后一个轴，也就是参数维度\n",
    "            mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "            normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n",
    "\n",
    "            inputs_shape = inputs.get_shape()\n",
    "            params_shape = inputs_shape[-1:]\n",
    "            beta = tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n",
    "            gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n",
    "            outputs = gamma * normalized + beta\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    # 对输入的one-hot使用矩阵处理，code的嵌入向量1\n",
    "    def get_input(self, one_hot_input):\n",
    "        input = tf.matmul(one_hot_input, self.Wi) + self.bi\n",
    "        return input\n",
    "\n",
    "    # 输出层，对h进行全连接输出\n",
    "    def get_output(self, h):\n",
    "        output = tf.nn.relu(tf.matmul(h, self.Wo) + self.bo)\n",
    "        return output\n",
    "\n",
    "    # 输出层，对h进行全连接输出\n",
    "    def get_final_output(self, h):\n",
    "        output = tf.nn.relu(tf.matmul(h, self.Wf) + self.bf)\n",
    "        return output\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, keep_prob =1.0,scope=\"scaled_dot_product_attention\"):\n",
    "\n",
    "        # Q: [N, T_q, d_k].\n",
    "        # K: [N, T_k, d_k].\n",
    "        # V: [N, T_k, d_v].\n",
    "\n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "            d_k = Q.get_shape().as_list()[-1]\n",
    "\n",
    "            # dot product\n",
    "            outputs = tf.matmul(Q, tf.transpose(K, [0, 2, 1]))  # (N, T_q, T_k)\n",
    "\n",
    "            # scale\n",
    "            outputs /= d_k ** 0.5\n",
    "\n",
    "            # softmax\n",
    "            outputs = tf.nn.softmax(outputs)\n",
    "\n",
    "            attention = tf.transpose(outputs, [0, 2, 1])\n",
    "            # 可视化attention\n",
    "            tf.summary.image(\"attention\", tf.expand_dims(attention[:1], -1))\n",
    "\n",
    "            # dropout\n",
    "            outputs = tf.nn.dropout(outputs, rate=keep_prob)\n",
    "\n",
    "            # weighted sum (context vectors)\n",
    "            outputs = tf.matmul(outputs, V)  # (N, T_q, d_v)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    # multihead_attention\n",
    "    def multihead_attention(self, queries, keys, values,num_heads=2,keep_prob=1.0,scope=\"multihead_attention\"):\n",
    "\n",
    "        d_model = queries.get_shape().as_list()[-1]\n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "            # Linear projections\n",
    "            Q = tf.layers.dense(queries, d_model,activation=tf.sigmoid, use_bias=True)  # (N, T_q, d_model)\n",
    "            K = tf.layers.dense(keys, d_model,activation=tf.sigmoid, use_bias=True)  # (N, T_k, d_model)\n",
    "            V = tf.layers.dense(values, d_model, activation=tf.sigmoid,use_bias=True)  # (N, T_k, d_model)\n",
    "\n",
    "            # Split and concat\n",
    "            Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)  # (h*N, T_q, d_model/h)\n",
    "            K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)  # (h*N, T_k, d_model/h)\n",
    "            V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)  # (h*N, T_k, d_model/h)\n",
    "\n",
    "            # Attention\n",
    "            outputs = self.scaled_dot_product_attention(Q_, K_, V_, keep_prob)\n",
    "\n",
    "            # Restore shape\n",
    "            outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)  # (N, T_q, d_model)\n",
    "\n",
    "            # Residual connection\n",
    "            outputs += queries\n",
    "\n",
    "            # Normalize\n",
    "            outputs = self.ln(outputs)\n",
    "        return outputs\n",
    "\n",
    "        # feedforward\n",
    "\n",
    "    def ff(self, inputs, num_units, scope=\"positionwise_feedforward\"):\n",
    "\n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "            # Inner layer\n",
    "            outputs = tf.layers.dense(inputs, num_units[0], activation=tf.nn.relu)\n",
    "\n",
    "            # Outer layer\n",
    "            outputs = tf.layers.dense(outputs, num_units[1])\n",
    "\n",
    "            # Residual connection\n",
    "            outputs += inputs\n",
    "\n",
    "            # Normalize\n",
    "            outputs = self.ln(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def TGRU_visit_cell(self, prev_h, concat_input):\n",
    "\n",
    "        # visit_length x batch_size x input_dim+info_dim+info_dim+1\n",
    "        batch_size = tf.shape(concat_input)[1]\n",
    "\n",
    "        # 只获取第一个时间\n",
    "        t = tf.slice(concat_input, [0, 0, 0], [1, batch_size, 1])\n",
    "        # 只获取第一个info\n",
    "        info = tf.slice(concat_input, [0, 0, 1], [1, batch_size, self.info_dim])\n",
    "\n",
    "        # 循环创建局部变量\n",
    "        for i in range(self.visit_length):\n",
    "            locals()['x' + str(i)] = tf.slice(concat_input, [i, 0, 1 + self.info_dim],\n",
    "                                              [1, batch_size, self.input_dim])\n",
    "\n",
    "        # 获得visit层的输入\n",
    "        visit_x = tf.zeros([batch_size, self.input_dim], dtype=tf.float32)\n",
    "        for i in range(self.visit_length):\n",
    "            visit_x += locals()['x' + str(i)]\n",
    "\n",
    "        visit_x = visit_x / self.visit_length\n",
    "\n",
    "        # 把输入和病人信息放在一个向量中\n",
    "        visit_x = tf.concat([visit_x, info], 2)\n",
    "        # 3维变2维 第一维是大小是1，没用\n",
    "        visit_x = tf.reshape(visit_x, [tf.shape(visit_x)[1], tf.shape(visit_x)[2]])\n",
    "        t = tf.reshape(t, [tf.shape(t)[1], tf.shape(t)[2]])\n",
    "\n",
    "        # visit嵌入向量经过GRU输出\n",
    "        ft = self.map_elapse_time(t, self.vhidden_dim)\n",
    "        z = tf.sigmoid(tf.matmul(visit_x, self.Wz_v) + tf.matmul(prev_h, self.Uz_v) + self.bz_v)\n",
    "        r = tf.sigmoid(tf.matmul(visit_x, self.Wr_v) + tf.matmul(prev_h, self.Ur_v) + self.br_v)\n",
    "        d = tf.matmul(ft, self.Wd_v)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(visit_x, self.Wh_v) + tf.matmul(r * h_, self.Uh_v) + self.bh_v)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    def encode(self, training=True):\n",
    "\n",
    "        # batch_size x code_num x input_dim\n",
    "        embeddings=self.get_input(self.one_hot_input)\n",
    "\n",
    "        with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            # embedding normalization\n",
    "            embeddings_norm = embeddings* (self.input_dim ** 0.5)\n",
    "            enc = tf.nn.dropout(embeddings_norm, self.keep_prob)\n",
    "\n",
    "            # Blocks n层attention\n",
    "            for i in range(self.attention_blocks_num):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n",
    "                    # self-attention\n",
    "                    enc = self.multihead_attention(queries=enc,keys=enc,values=enc,num_heads=self.num_heads,keep_prob=self.keep_prob)\n",
    "                    # feed forward\n",
    "                    d_model = enc.get_shape().as_list()[-1]\n",
    "                    enc = self.ff(enc, num_units=[self.f_dim, d_model])\n",
    "        # enc (N, T1, d_model)\n",
    "        memory = enc\n",
    "        return memory\n",
    "\n",
    "\n",
    "    def TGRU_visit(self):\n",
    "\n",
    "        # batch_size x len x d_model)\n",
    "        encode = self.encode()\n",
    "        # visit_len x batch_size x d_model\n",
    "        encode=tf.transpose(encode, [1,0,2])\n",
    "\n",
    "        batch_size = tf.shape(encode)[1]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "\n",
    "        # make info [batch_size x seq_length x info_dim] --> [seq_length x batch_size x info_dim]\n",
    "        scan_info = tf.transpose(self.info, [1, 0, 2])\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "\n",
    "        # [seq_length x batch_size x input_dim+info_dim+1]\n",
    "        concat_input = tf.concat([scan_time, scan_info, encode], 2)\n",
    "\n",
    "        # 已经知道了visit的个数和长度，可以直接把concat_input拆分成visit个\n",
    "        # [visit_num x visit_length x batch_size x input_dim+info_dim_1]\n",
    "        initial_hidden = tf.zeros([batch_size, self.vhidden_dim], tf.float32)\n",
    "        concat_input = tf.reshape(concat_input, [self.visit_num, self.visit_length, tf.shape(concat_input)[1],\n",
    "                                                 tf.shape(concat_input)[2]])\n",
    "        visit_h = tf.scan(self.TGRU_visit_cell, concat_input, initializer=initial_hidden, name='visit')\n",
    "\n",
    "        return visit_h\n",
    "\n",
    "    def get_cross_loss(self):\n",
    "        output = self.TGRU_visit()\n",
    "        batch_size = tf.shape(output)[1]\n",
    "        final_output = tf.map_fn(self.get_output, output)\n",
    "        fo = tf.zeros([batch_size, self.output_dim], dtype=tf.float32)\n",
    "        for i in range(self.visit_num ):\n",
    "            fo=fo+final_output[i]\n",
    "        fo=fo/self.visit_num\n",
    "        fo = tf.nn.dropout(fo, self.keep_prob)\n",
    "\n",
    "        pre_output=self.get_final_output(fo)\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=pre_output))\n",
    "\n",
    "        y_pred = tf.argmax(pre_output, 1)\n",
    "        y = tf.argmax(self.labels, 1)\n",
    "\n",
    "        return cross_entropy, y_pred, y, pre_output, self.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch生成器\n",
    "# bacth中有cutting和padding，由于长度不同，因此开始学习的时候可以每个数据单独学习\n",
    "# 生成的batch里的病人visit和code的个数一样\n",
    "def batch_generator(outFile, visit_num, code_num, patient_num, batch_size):\n",
    "    patient_code_file=open(outFile + '/patient_code' + '.seqs','rb')\n",
    "    patient_code=pickle.load(patient_code_file)\n",
    "    \n",
    "    codespatientsinfo_file=open(outFile + '/codespatientsinfo' + '.seqs','rb')\n",
    "    codespatientsinfo=pickle.load(codespatientsinfo_file)\n",
    "    \n",
    "    visit_delt_dates_file=open(outFile + '/visit_delt_dates' + '.seqs','rb')\n",
    "    visit_delt_dates=pickle.load(visit_delt_dates_file)\n",
    "    \n",
    "    code_delt_dates_file=open(outFile + '/code_delt_dates' + '.seqs','rb')\n",
    "    code_delt_dates=pickle.load(code_delt_dates_file)\n",
    "    \n",
    "    visits_num_file=open(outFile + '/visits_num' + '.seqs','rb')\n",
    "    visits_num=pickle.load(visits_num_file)\n",
    "    \n",
    "    codes_num_file=open(outFile + '/codes_num' + '.seqs','rb')\n",
    "    codes_num=pickle.load(codes_num_file)\n",
    "    \n",
    "    death_labels_file=open(outFile + '/death_labels' + '.seqs','rb')\n",
    "    death_labels=pickle.load(death_labels_file)\n",
    "    \n",
    "    patient_code_file.close()\n",
    "    codespatientsinfo_file.close()\n",
    "    visit_delt_dates_file.close()\n",
    "    code_delt_dates_file.close()\n",
    "    visits_num_file.close()\n",
    "    codes_num_file.close()\n",
    "\n",
    "    batch_patient_code=[]\n",
    "    batch_code_delt_dates=[]\n",
    "    batch_codespatientsinfo=[]\n",
    "    batch_visit_delt_dates=[]\n",
    "    batch_labels=[]\n",
    "\n",
    "    \n",
    "    # padding and cutting\n",
    "    for i in range(batch_size):\n",
    "        j=random.randint(0, patient_num-1)\n",
    "        if visits_num[j][0]== visit_num:\n",
    "           # print 'pick',j\n",
    "            code=[]\n",
    "            code_date=[]\n",
    "            code_info=[]\n",
    "            for k in range(visit_num):\n",
    "                if  codes_num[j][k]> code_num:\n",
    "                    if k==0: \n",
    "                        code.extend(patient_code[j][0:code_num])\n",
    "                        code_date.extend(code_delt_dates[j][0:code_num])\n",
    "                        code_info.extend(codespatientsinfo[j][0:code_num])\n",
    "                    else:\n",
    "                        start=k*codes_num[j][k-1]\n",
    "                        code.extend(patient_code[j][start :start+code_num])\n",
    "                        code_date.extend(code_delt_dates[j][start:start+code_num])\n",
    "                        code_info.extend(codespatientsinfo[j][start:start+code_num])\n",
    "                elif codes_num[j][k] < code_num:\n",
    "                    if k==0:\n",
    "                        code.extend(patient_code[j][:codes_num[j][k]])\n",
    "                        code.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_date.extend(code_delt_dates[j][:codes_num[j][k]])\n",
    "                        code_date.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_info.extend(codespatientsinfo[j][:codes_num[j][k]])\n",
    "                        code_info.extend([[0,0]]*(code_num-codes_num[j][k]))\n",
    "                    else:\n",
    "                        start2=0\n",
    "                        for n in range(k):\n",
    "                            start2+=codes_num[j][n]\n",
    "                        code.extend(patient_code[j][start2: start2+codes_num[j][k]])\n",
    "                        code.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_date.extend(code_delt_dates[j][start2:start2+codes_num[j][k]])\n",
    "                        code_date.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_info.extend(codespatientsinfo[j][start2:start2+codes_num[j][k]])\n",
    "                        code_info.extend([[0,0]]*(code_num-codes_num[j][k]))\n",
    "                else:\n",
    "                    code.extend(patient_code[j])\n",
    "                    code_date.extend(code_delt_dates[j])\n",
    "                    code_info.extend(codespatientsinfo[j])\n",
    "            batch_patient_code.append(code)\n",
    "            batch_code_delt_dates.append(code_date)  \n",
    "            batch_codespatientsinfo.append(code_info)\n",
    "            batch_labels.append(death_labels[j])\n",
    "                    \n",
    "        \n",
    "        # 考虑随机生成的时候可能有的数据始终选不到，因此还要有一个顺序生成\n",
    "    return batch_patient_code, batch_codespatientsinfo, batch_code_delt_dates,batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "learning_rate = 1e-3\n",
    "iters = 20\n",
    "dropout=1.0\n",
    "\n",
    "# 网络参数\n",
    "info_dim=2\n",
    "icd9_num=259\n",
    "one_hot_input_dim=icd9_num # 数据len(types)=259, one-hot的长度应该是259\n",
    "input_dim = 200 \n",
    "hidden_dim1 =150\n",
    "output_dim1=150\n",
    "hidden_dim2=150\n",
    "output_dim2=150\n",
    "vhidden_dim=100\n",
    "voutput_dim=150\n",
    "output_dim=200\n",
    "\n",
    "# 生成batch数据参数\n",
    "outFile='SEQ'\n",
    "checkpoint_dir_pre='MODEL_pre'\n",
    "result_dir='RESULT'\n",
    "batch_size=4\n",
    "visit_num=2 #visit_num=random.randint() \n",
    "code_num=10 #code_num=random.randint() # code_num=visit_length\n",
    "patient_num=14 #病人的总个数\n",
    "\n",
    "\n",
    "\n",
    "# 数据集获取\n",
    "patient_code_file=open(outFile + '/patient_code' + '.seqs','rb')\n",
    "patient_code=pickle.load(patient_code_file)\n",
    "\n",
    "code_delt_dates_file=open(outFile + '/code_delt_dates' + '.seqs','rb')\n",
    "code_delt_dates=pickle.load(code_delt_dates_file)\n",
    "\n",
    "visit_delt_dates_file=open(outFile + '/visit_delt_dates' + '.seqs','rb')\n",
    "visit_delt_dates=pickle.load(visit_delt_dates_file)\n",
    "\n",
    "visits_num_file=open(outFile + '/visits_num' + '.seqs','rb')\n",
    "visits_num=pickle.load(visits_num_file)\n",
    "\n",
    "codes_num_file=open(outFile + '/codes_num' + '.seqs','rb')\n",
    "codes_num=pickle.load(codes_num_file)\n",
    "\n",
    "visitspatientsinfo_file=open(outFile + '/visitspatientsinfo' + '.seqs','rb')\n",
    "visitspatientsinfo=pickle.load(visitspatientsinfo_file)\n",
    "\n",
    "codespatientsinfo_file=open(outFile + '/codespatientsinfo' + '.seqs','rb')\n",
    "codespatientsinfo=pickle.load(codespatientsinfo_file)\n",
    "\n",
    "death_labels_file=open(outFile + '/death_labels' + '.seqs','rb')\n",
    "death_labels=pickle.load(death_labels_file)\n",
    "\n",
    "\n",
    "patient_code_file.close()\n",
    "code_delt_dates_file.close()\n",
    "visit_delt_dates_file.close()\n",
    "visits_num_file.close()\n",
    "codes_num_file.close()\n",
    "visitspatientsinfo_file.close()\n",
    "codespatientsinfo_file.close()\n",
    "death_labels_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MTGRU_predeath3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2dafb4ec08a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 实例化网络\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m mtgrupredeath3 = MTGRU_predeath3(visit_num, code_num, one_hot_input_dim, input_dim, info_dim, output_dim1,\n\u001b[0m\u001b[1;32m      3\u001b[0m                  output_dim2, voutput_dim,output_dim, vhidden_dim, hidden_dim1, hidden_dim2,visits_num,codes_num,visit_delt_dates,visitspatientsinfo,patient_num,patient_code,code_delt_dates)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MTGRU_predeath3' is not defined"
     ]
    }
   ],
   "source": [
    "# 实例化网络\n",
    "mtgrupredeath3 = MTGRU_predeath3(visit_num, code_num, one_hot_input_dim, input_dim, info_dim, output_dim1,\n",
    "                 output_dim2, voutput_dim,output_dim, vhidden_dim, hidden_dim1, hidden_dim2,visits_num,codes_num,visit_delt_dates,visitspatientsinfo,patient_num,patient_code,code_delt_dates)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标，使用交叉熵/ SME\n",
    "cross_entropy, y_pred, y, logits, labels = mtgrupredeath3.get_cross_loss()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "init = tf.global_variables_initializer()\n",
    "sess= tf.Session() \n",
    "sess.run(init)\n",
    "Loss = np.zeros(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss, Accuracy, AUC, AUC Macro: 0.000002 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000002 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n",
      "Loss, Accuracy, AUC, AUC Macro: 0.000001 1.000000 1.000000 1.000000\n"
     ]
    }
   ],
   "source": [
    " # 生成batch训练\n",
    "for i in range(iters):\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    Labels = []\n",
    "    Logits = []\n",
    "    LOSS = 0 \n",
    "    \n",
    "    n=0\n",
    "    for i in range(10): #学习完整个数据库需要的次数,要按照实际\n",
    "        \n",
    "        # 生成batch\n",
    "        xindex, info, t,batch_labels = batch_generator(outFile, visit_num, code_num, patient_num,batch_size)\n",
    "        if len(xindex)==0: continue\n",
    "        _, c_train, y_pred_train, y_train, logits_train, labels_train = sess.run([optimizer, cross_entropy, y_pred, y, logits, labels ], feed_dict={mtgrupredeath3.inputindex: xindex, mtgrupredeath3.time: t, mtgrupredeath3.info:info,mtgrupredeath3.labels:batch_labels,mtgrupredeath3.keep_prob:dropout})\n",
    "        LOSS += c_train\n",
    "        \n",
    "        if n > 0:\n",
    "            Y_true = np.concatenate([Y_true, y_train], 0)\n",
    "            Y_pred = np.concatenate([Y_pred, y_pred_train], 0)\n",
    "            Labels = np.concatenate([Labels, labels_train], 0)\n",
    "            Logits = np.concatenate([Logits, logits_train], 0)\n",
    "        else:\n",
    "            Y_true = y_train\n",
    "            Y_pred = y_pred_train\n",
    "            Labels = labels_train\n",
    "            Logits = logits_train\n",
    "        n+=1\n",
    "    \n",
    "    Loss[i] = LOSS / 10\n",
    "    \n",
    "    \n",
    "    total_acc = accuracy_score(Y_true, Y_pred)\n",
    "    total_auc = roc_auc_score(Labels, Logits, average='micro')\n",
    "    total_auc_macro = roc_auc_score(Labels, Logits, average='macro')\n",
    "    \n",
    "    print('Loss, Accuracy, AUC, AUC Macro: %f %f %f %f' %(Loss[i],total_acc,total_auc,total_auc_macro))\n",
    "    \n",
    "    #print('Loss %f' %(Loss[i]))\n",
    "    #print(\"Train Accuracy = {:.3f}\".format(total_acc))\n",
    "    #print(\"Train AUC = {:.3f}\".format(total_auc))\n",
    "    #print(\"Train AUC Macro = {:.3f}\".format(total_auc_macro)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在整个数据集上的准确度\n",
    "pres, codeAttention, visitAttention  = sess.run(mtgrupredeath3.get_all_Pre_sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有数据的准确度\n",
    "def meanAcc(pres):\n",
    "    test_pres=[]\n",
    "    test_death_labels=[]\n",
    "    for i in range(len(pres)):\n",
    "        if pres[i][0][0]>pres[i][0][1]: test_pres.append(0)\n",
    "        else: test_pres.append(1)\n",
    "        test_death_labels.append(death_labels[i][1])\n",
    "\n",
    "    format_pres=[]\n",
    "    for pre in pres:\n",
    "        format_pres.append(pre[0][1])\n",
    "\n",
    "    all_acc = accuracy_score(test_death_labels, test_pres)\n",
    "    all_auc = roc_auc_score(test_death_labels, format_pres, average='micro')\n",
    "    all_auc_macro = roc_auc_score(test_death_labels, format_pres, average='macro')\n",
    "    return all_acc,all_auc,all_auc_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个病人的每个visit下最高贡献的code，死亡病人每个visit下最高贡献code\n",
    "def PatientImpcode(codeAttention,patientAttention):\n",
    "    all_codes_num_file= open(outFile + '/all_codes_num' + '.seqs', 'rb')\n",
    "    all_codes_num=pickle.load(all_codes_num_file) \n",
    "    codes_num_file= open(outFile + '/codes_num' + '.seqs', 'rb')\n",
    "    codes_num=pickle.load(codes_num_file) \n",
    "    newSeqs_file= open(outFile + '/newSeqs' + '.seqs', 'rb')\n",
    "    newSeqs=pickle.load(newSeqs_file) \n",
    "    types_file= open(outFile + '/types2' + '.seqs', 'rb')\n",
    "    types=pickle.load(types_file) \n",
    "    type_pids_file= open(outFile + '/type_pids2' + '.seqs', 'rb')\n",
    "    type_pids=pickle.load(type_pids_file)\n",
    "    death_labels_file=open(outFile + '/death_labels' + '.seqs','rb')\n",
    "    death_labels=pickle.load(death_labels_file)\n",
    "    visits_num_file=open(outFile + '/visits_num' + '.seqs','rb')\n",
    "    visits_num=pickle.load(visits_num_file)\n",
    "    newSeqs_file.close()\n",
    "    types_file.close()\n",
    "    type_pids_file.close()\n",
    "    all_codes_num_file.close()\n",
    "    death_labels_file.close()\n",
    "    visits_num_file.close()\n",
    "\n",
    "    #获取每个patient下的code的attention,每个visit的attention\n",
    "    code_attentions_value=[]\n",
    "    visit_attentions_value=[]\n",
    "    for i in range(patient_num):\n",
    "        code_attention_value=[]\n",
    "        visit_attention_value=[]\n",
    "        n=0\n",
    "        for j in range(len(codes_num[i])):\n",
    "            av=[]\n",
    "            for k in range(codes_num[i][j]):\n",
    "                av.append(np.linalg.norm(codeAttention[i][n]))\n",
    "                n+=1\n",
    "            code_attention_value.append(av)\n",
    "        code_attentions_value.append(code_attention_value)\n",
    "        for k in range(visits_num[i][0]):\n",
    "            visit_attention_value.append(np.linalg.norm(visitAttention[i][k]))\n",
    "        visit_attentions_value.append(visit_attention_value)\n",
    "\n",
    "    # 对每个patient下的visit下code的attention大小进行排名,visit的attention大小排名\n",
    "    code_attentions_rank=[]\n",
    "    visit_attentions_rank=[]\n",
    "    for i in range(patient_num):\n",
    "        code_attention_rank=[]\n",
    "        for j in range(len(codes_num[i])):\n",
    "            rank=np.argsort(code_attentions_value[i][j])\n",
    "            if j ==0 : code_attention_rank.append(rank[::-1])\n",
    "            else:  code_attention_rank.append(rank[::-1]+codes_num[i][j-1])\n",
    "        code_attentions_rank.append(code_attention_rank)\n",
    "        rank2=np.argsort(visit_attentions_value[i])\n",
    "        visit_attentions_rank.append(rank2[::-1])\n",
    "\n",
    "\n",
    "    # 病人id对应的重要code的icd9编码\n",
    "    PidImpcodeMap={}\n",
    "    deathPidImpcodeMap={}\n",
    "    PidMostImpcodeMap={}\n",
    "    deathPidMostImpcodeMap={}\n",
    "    longAffcode=[]\n",
    "    for i in range(len(code_attentions_rank)):\n",
    "        a=[]\n",
    "        impCode=[]\n",
    "        for j in range(len(code_attentions_rank[i])):\n",
    "            a.append(types[patient_code[i][code_attentions_rank[i][j][0]]])\n",
    "        impCode.append(a)\n",
    "        PidImpcodeMap[type_pids[i]]=impCode\n",
    "        PidMostImpcodeMap[type_pids[i]]=impCode[0][visit_attentions_rank[i][0]]\n",
    "        # 死亡\n",
    "        if death_labels[i][1]==1:\n",
    "            deathPidImpcodeMap[type_pids[i]]=impCode\n",
    "            deathPidMostImpcodeMap[type_pids[i]]=impCode[0][visit_attentions_rank[i][0]]\n",
    "        # 长期影响的重要code（最重要的visit不是最后一个）\n",
    "        if visit_attentions_rank[i][0] != (visits_num[i][0]-1) : longAffcode.append(impCode[0][visit_attentions_rank[i][0]])\n",
    "\n",
    "    return PidImpcodeMap,deathPidImpcodeMap,PidMostImpcodeMap,deathPidMostImpcodeMap,longAffcode\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, AUC, AUC Macro: 0.785714 0.765306 0.765306\n"
     ]
    }
   ],
   "source": [
    "all_acc,all_auc,all_auc_macro=meanAcc(pres)\n",
    "PidImpcodeMap,deathPidImpcodeMap,PidMostImpcodeMap,deathPidMostImpcodeMap,longAffcode=PatientImpcode(codeAttention,patientAttention)\n",
    "    \n",
    "print('Accuracy, AUC, AUC Macro: %f %f %f' %(all_acc,all_auc,all_auc_macro))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10059: [['D_572.3', 'D_572.3']],\n",
       " 10094: [['D_250.00', 'D_244.9']],\n",
       " 10117: [['D_287.5', 'D_V42.81']],\n",
       " 10124: [['D_250.00', 'D_599.0']],\n",
       " 40310: [['D_707.03', 'D_227.3']],\n",
       " 42135: [['D_303.93', 'D_585.9']],\n",
       " 42346: [['D_397.0', 'D_458.9']]}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deathPidImpcodeMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10059: 'D_572.3',\n",
       " 10094: 'D_244.9',\n",
       " 10117: 'D_V42.81',\n",
       " 10124: 'D_599.0',\n",
       " 40310: 'D_227.3',\n",
       " 42135: 'D_585.9',\n",
       " 42346: 'D_458.9'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deathPidMostImpcodeMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "肝脓肿与慢性肝病后遗症,门脉高压\n",
    "甲状腺功能减退\n",
    "骨髓移植\n",
    "尿路感染\n",
    "垂体和颅咽管良性肿瘤\n",
    "慢性肾病\n",
    "低血压"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D_284.1', 'D_038.3']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longAffcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全血细胞减少症又名再生障碍性贫血(再障)，是骨髓造血功能衰竭所导致的一种全血减少综合征\n",
    "厌氧菌败血症"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xindex, info, t,batch_labels = batch_generator(outFile, visit_num, code_num, patient_num,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[98,\n",
       "  3,\n",
       "  0,\n",
       "  99,\n",
       "  100,\n",
       "  49,\n",
       "  8,\n",
       "  101,\n",
       "  102,\n",
       "  53,\n",
       "  108,\n",
       "  3,\n",
       "  109,\n",
       "  110,\n",
       "  102,\n",
       "  25,\n",
       "  111,\n",
       "  112,\n",
       "  20,\n",
       "  11],\n",
       " [140,\n",
       "  3,\n",
       "  41,\n",
       "  141,\n",
       "  102,\n",
       "  8,\n",
       "  63,\n",
       "  142,\n",
       "  143,\n",
       "  10,\n",
       "  47,\n",
       "  61,\n",
       "  8,\n",
       "  145,\n",
       "  63,\n",
       "  48,\n",
       "  10,\n",
       "  127,\n",
       "  0,\n",
       "  0],\n",
       " [38, 39, 25, 31, 40, 41, 42, 43, 35, 20, 44, 25, 45, 46, 11, 0, 0, 0, 0, 0],\n",
       " [98,\n",
       "  3,\n",
       "  0,\n",
       "  99,\n",
       "  100,\n",
       "  49,\n",
       "  8,\n",
       "  101,\n",
       "  102,\n",
       "  53,\n",
       "  108,\n",
       "  3,\n",
       "  109,\n",
       "  110,\n",
       "  102,\n",
       "  25,\n",
       "  111,\n",
       "  112,\n",
       "  20,\n",
       "  11]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
