{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import cPickle as pickle\n",
    "import h5py\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTGRU_precode(object):\n",
    "\n",
    "    def init_weights(self, input_dim, output_dim, name=None, std=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=std / math.sqrt(input_dim)), name=name)\n",
    "\n",
    "    def init_bias(self, output_dim, name=None):\n",
    "        return tf.Variable(tf.zeros([output_dim]), name=name)\n",
    "\n",
    "    def __init__(self, visit_num, visit_length, one_hot_input_dim, input_dim, info_dim, output_dim1,\n",
    "                 output_dim2, voutput_dim,output_dim, vhidden_dim, hidden_dim1, hidden_dim2,patient_visit_num,patient_visit_length,visit_time,visit_info,patient_num,code_inputindex,code_time):\n",
    "\n",
    "        #参数patient_visit_num, patient_visit_length, visit_time, visit_info, patient_num, code_inputindex, code_time用来对新的 patient预测疾病\n",
    "        # 一个病人visit的个数\n",
    "        self.visit_num = visit_num\n",
    "        # 一个visit中的code的个数\n",
    "        self.visit_length = visit_length\n",
    "\n",
    "        # 输入的one-hot的维度\n",
    "        self.one_hot_input_dim = one_hot_input_dim\n",
    "        # one-hot变换后作为网络输入的维度\n",
    "        self.input_dim = input_dim\n",
    "        # 患者信息\n",
    "        self.info_dim = info_dim\n",
    "\n",
    "\n",
    "        # 中间层输出,每层gru有一个输出，visit层有一个输出\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "        self.voutput_dim = voutput_dim\n",
    "        # 中间隐藏层，四个gru层，一个visit层\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.vhidden_dim = vhidden_dim\n",
    "\n",
    "        # 最后一层gru最终输出\n",
    "        self.output_dim = output_dim\n",
    "        final_output_dim=one_hot_input_dim\n",
    "        self.final_output_dim=one_hot_input_dim\n",
    "\n",
    "        # one-hot后的矩阵参数\n",
    "        self.Wi = self.init_weights(one_hot_input_dim, input_dim, name='OneHot_w')\n",
    "        self.bi = self.init_bias(input_dim, name='OneHot_w')\n",
    "\n",
    "        # encoder的GRU参数\n",
    "        self.Wz_enc = self.init_weights(input_dim, hidden_dim1, name='Update_wx_enc')\n",
    "        self.Uz_enc = self.init_weights(hidden_dim1, hidden_dim1, name='Update_wh_enc')\n",
    "        self.bz_enc = self.init_bias(hidden_dim1, name='Update_bias_enc')\n",
    "\n",
    "        self.Wz_enc2 = self.init_weights(output_dim1, hidden_dim2, name='Update_wx_enc2')\n",
    "        self.Uz_enc2 = self.init_weights(hidden_dim2, hidden_dim2, name='Update_wh_enc2')\n",
    "        self.bz_enc2 = self.init_bias(hidden_dim2, name='Update_bias_enc2')\n",
    "\n",
    "        self.Wr_enc = self.init_weights(input_dim, hidden_dim1, name='Reset_wx_enc')\n",
    "        self.Ur_enc = self.init_weights(hidden_dim1, hidden_dim1, name='Reset_wh_enc')\n",
    "        self.br_enc = self.init_bias(hidden_dim1, name='Reset_bias_enc')\n",
    "\n",
    "        self.Wr_enc2 = self.init_weights(output_dim1, hidden_dim2, name='Reset_wx_enc2')\n",
    "        self.Ur_enc2 = self.init_weights(hidden_dim2, hidden_dim2, name='Reset_wh_enc2')\n",
    "        self.br_enc2 = self.init_bias(hidden_dim2, name='Reset_bias_enc2')\n",
    "\n",
    "        self.Wd_enc = tf.ones([1, self.hidden_dim1], dtype=tf.float32, name='Decay_w_enc')\n",
    "        self.Wd_enc2 = tf.ones([1, self.hidden_dim2], dtype=tf.float32, name='Decay_w_enc2')\n",
    "\n",
    "        self.Wh_enc = self.init_weights(self.input_dim, self.hidden_dim1, name='Canditateh_wx_enc')\n",
    "        self.Uh_enc = self.init_weights(self.hidden_dim1, self.hidden_dim1, name='Canditateh_wh_enc')\n",
    "        self.bh_enc = self.init_bias(self.hidden_dim1, name='Canditateh_bias_enc')\n",
    "\n",
    "        self.Wh_enc2 = self.init_weights(self.output_dim1, self.hidden_dim2, name='Canditateh_wx_enc2')\n",
    "        self.Uh_enc2 = self.init_weights(self.hidden_dim2, self.hidden_dim2, name='Canditateh_wh_enc2')\n",
    "        self.bh_enc2 = self.init_bias(self.hidden_dim2, name='Canditateh_bias_enc2')\n",
    "\n",
    "\n",
    "        # visit层的参数\n",
    "        # 输入是上一层的输出加上病人信息\n",
    "        self.Wz_v = self.init_weights(output_dim2 + info_dim, vhidden_dim, name='Update_wx_v')\n",
    "        self.Uz_v = self.init_weights(vhidden_dim, vhidden_dim, name='Update_wh_v')\n",
    "        self.bz_v = self.init_bias(vhidden_dim, name='Update_bias_dec')\n",
    "\n",
    "        self.Wr_v = self.init_weights(output_dim2 + info_dim, vhidden_dim, name='Reset_wx_v')\n",
    "        self.Ur_v = self.init_weights(vhidden_dim, vhidden_dim, name='Reset_wh_v')\n",
    "        self.br_v = self.init_bias(vhidden_dim, name='Reset_bias_v')\n",
    "\n",
    "        self.Wd_v = tf.ones([1, self.vhidden_dim], dtype=tf.float32, name='Decay_w_v')\n",
    "\n",
    "        self.Wh_v = self.init_weights(self.output_dim2 + info_dim, self.vhidden_dim, name='Canditateh_wx_v')\n",
    "        self.Uh_v = self.init_weights(self.vhidden_dim, self.vhidden_dim, name='Canditateh_wh_v')\n",
    "        self.bh_v = self.init_bias(self.vhidden_dim, name='Canditateh_bias_v')\n",
    "\n",
    "\n",
    "        # 输出层\n",
    "        # encoder1\n",
    "        self.Wo1 = self.init_weights(hidden_dim1, output_dim1, name='Output1_weight')\n",
    "        self.bo1 = self.init_bias(output_dim1, name='Output1_bias')\n",
    "        # encoder2\n",
    "        self.Wo2 = self.init_weights(hidden_dim2, output_dim2, name='Output2_weight')\n",
    "        self.bo2 = self.init_bias(output_dim2, name='Output2_bias')\n",
    "        # visit层的输出\n",
    "        self.Wo = self.init_weights(vhidden_dim, output_dim, name='Output_w')\n",
    "        self.bo = self.init_bias(output_dim, name='Output_bias')\n",
    "        # 最终输出全连接\n",
    "        self.Wf = self.init_weights(output_dim, final_output_dim, name='Final_output_w')\n",
    "        self.bf = self.init_bias(final_output_dim, name='Final_output_bias')\n",
    "\n",
    "\n",
    "        # 输入占位符\n",
    "        # [batch size x seq length x input dim]\n",
    "        self.inputindex=tf.placeholder(dtype=tf.int32,shape=[None,None])\n",
    "        self.one_hot_input=tf.one_hot(indices=self.inputindex, depth=self.one_hot_input_dim, axis=2) #输入\n",
    "        self.lable_multi_hot=tf.placeholder('float',shape=[None,final_output_dim]) #标签\n",
    "        self.time = tf.placeholder('float', [None, None])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.info = tf.placeholder('float', shape=[None, None, self.info_dim])# 患者人口信息\n",
    "\n",
    "\n",
    "        # 对新的人来预测疾病\n",
    "        self.patient_visit_num = patient_visit_num # 一个患者的visit个数\n",
    "        self.patient_visit_length = patient_visit_length # 一个患者的visit中的code个数\n",
    "        self.visit_time = visit_time\n",
    "        self.visit_info = visit_info\n",
    "        self.patient_num=patient_num\n",
    "        self.code_inputindex=code_inputindex #一个病人所有code的index\n",
    "        self.code_time=code_time\n",
    "\n",
    "\n",
    "\n",
    "    # 对输入的one-hot使用矩阵处理，code的嵌入向量1\n",
    "    def get_input(self, one_hot_input):\n",
    "        input = tf.matmul(one_hot_input, self.Wi) + self.bi\n",
    "        return input\n",
    "\n",
    "    # encoder第一层TGRU cell，code的嵌入向量2\n",
    "    def TGRU_encoder_cell1(self, prev_h, concat_input):\n",
    "\n",
    "        # concat_input:[batch_size x input_dim+1]\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "        x = tf.slice(concat_input, [0, 1], [batch_size, self.input_dim])\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "\n",
    "        ft = self.map_elapse_time(t, self.hidden_dim1)\n",
    "\n",
    "        z = tf.sigmoid(tf.matmul(x, self.Wz_enc) + tf.matmul(prev_h, self.Uz_enc) + self.bz_enc)\n",
    "        r = tf.sigmoid(tf.matmul(x, self.Wr_enc) + tf.matmul(prev_h, self.Ur_enc) + self.br_enc)\n",
    "        d = tf.matmul(ft, self.Wd_enc)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(x, self.Wh_enc) + tf.matmul(r * h_, self.Uh_enc) + self.bh_enc)\n",
    "\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    # encoder第二层TGRU cell\n",
    "    def TGRU_encoder_cell2(self, prev_h, concat_input):\n",
    "\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "        x = tf.slice(concat_input, [0, 1], [batch_size, self.output_dim1])\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "\n",
    "        ft = self.map_elapse_time(t, self.hidden_dim2)\n",
    "\n",
    "        z = tf.sigmoid(tf.matmul(x, self.Wz_enc2) + tf.matmul(prev_h, self.Uz_enc2) + self.bz_enc2)\n",
    "        r = tf.sigmoid(tf.matmul(x, self.Wr_enc2) + tf.matmul(prev_h, self.Ur_enc2) + self.br_enc2)\n",
    "        d = tf.matmul(ft, self.Wd_enc2)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(x, self.Wh_enc2) + tf.matmul(r * h_, self.Uh_enc2) + self.bh_enc2)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "    # visit层的TGURU cell\n",
    "    # 此时的输入与其他cell的不同，concat_input应该是多个时刻的code的，而其他的输入都是一个时刻的code，因此在后面对get_encoder_h2输出的结果做处理\n",
    "    # 此时的concat_input是一个三维向量，也就是有多时刻的code，每个时刻有一个时间和batch个数个code\n",
    "    # [visit_length x batch_size x 1+input_dim ]\n",
    "    # 创建每个visit的长度的参数，来方便创建多个输入x\n",
    "    def TGRU_visit_cell(self, prev_h, concat_input):\n",
    "\n",
    "        # visit_length x batch_size x input_dim+info_dim+info_dim+1\n",
    "        batch_size = tf.shape(concat_input)[1]\n",
    "\n",
    "        # 只获取第一个时间\n",
    "        t = tf.slice(concat_input, [0, 0, 0], [1, batch_size, 1])\n",
    "        # 只获取第一个info\n",
    "        info = tf.slice(concat_input, [0, 0, 1], [1, batch_size, self.info_dim])\n",
    "\n",
    "        # 循环创建局部变量\n",
    "        for i in range(self.visit_length):\n",
    "            locals()['x' + str(i)] = tf.slice(concat_input, [i, 0, 1 + self.info_dim],\n",
    "                                              [1, batch_size, self.output_dim2])\n",
    "\n",
    "        # 获得visit层的输入\n",
    "        visit_x = tf.zeros([batch_size, self.output_dim2], dtype=tf.float32)\n",
    "        for i in range(self.visit_length):\n",
    "            visit_x += locals()['x' + str(i)]\n",
    "\n",
    "        visit_x = visit_x / self.visit_length\n",
    "\n",
    "        # 把输入和病人信息放在一个向量中\n",
    "        visit_x = tf.concat([visit_x, info], 2)\n",
    "        # 3维变2维 第一维是大小是1，没用\n",
    "        visit_x = tf.reshape(visit_x,[tf.shape(visit_x)[1],tf.shape(visit_x)[2]])\n",
    "        t = tf.reshape(t,[tf.shape(t)[1],tf.shape(t)[2]])\n",
    "\n",
    "\n",
    "        # visit嵌入向量经过GRU输出\n",
    "        ft = self.map_elapse_time(t, self.vhidden_dim)\n",
    "        z = tf.sigmoid(tf.matmul(visit_x, self.Wz_v) + tf.matmul(prev_h, self.Uz_v) + self.bz_v)\n",
    "        r = tf.sigmoid(tf.matmul(visit_x, self.Wr_v) + tf.matmul(prev_h, self.Ur_v) + self.br_v)\n",
    "        d = tf.matmul(ft, self.Wd_v)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(visit_x, self.Wh_v) + tf.matmul(r * h_, self.Uh_v) + self.bh_v)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "\n",
    "\n",
    "    # 每个输出层的操作\n",
    "    # encoder1的输出\n",
    "    def get_output1(self, h):\n",
    "        output = tf.matmul(h, self.Wo1) + self.bo1\n",
    "        # output = tf.nn.softmax(tf.nn.relu(tf.matmul(state, self.Wo) + self.bo))\n",
    "        return output\n",
    "\n",
    "    # encoder2的输出\n",
    "    def get_output2(self, h):\n",
    "        output = tf.matmul(h, self.Wo2) + self.bo2\n",
    "        # output = tf.nn.softmax(tf.nn.relu(tf.matmul(state, self.Wo) + self.bo))\n",
    "        return output\n",
    "\n",
    "    # 输出层，对h进行全连接输出\n",
    "    def get_output(self, h):\n",
    "        output = tf.nn.relu(tf.matmul(h, self.Wo) + self.bo)\n",
    "        output = tf.nn.dropout(output, self.keep_prob)\n",
    "        output = tf.matmul(output, self.Wf) + self.bf\n",
    "        return output\n",
    "\n",
    "\n",
    "    # encoder1的输出h\n",
    "    def get_encoder1_h(self):  # Returns all hidden states for the samples in a batch\n",
    "\n",
    "        convert_input = self.get_input(self.one_hot_input)\n",
    "        batch_size = tf.shape(convert_input)[0]\n",
    "        scan_input = tf.transpose(convert_input, perm=[1, 0, 2])  # scan input is [seq_length x batch_size x input_dim]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim1], tf.float32)\n",
    "\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        concat_input = tf.concat([scan_time, scan_input], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder1_h = tf.scan(self.TGRU_encoder_cell1, concat_input, initializer=initial_hidden, name='encoder1_h')\n",
    "\n",
    "        return encoder1_h\n",
    "\n",
    "    # encoder2的输出h\n",
    "    def get_encoder2_h(self):  # Returns all hidden states for the samples in a batch\n",
    "\n",
    "        encoder1_h = self.get_encoder1_h()\n",
    "        encoder1_output = tf.map_fn(self.get_output1, encoder1_h)\n",
    "\n",
    "        batch_size = tf.shape(encoder1_h)[1]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim2], tf.float32)\n",
    "\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        concat_input = tf.concat([scan_time, encoder1_output], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder2_h = tf.scan(self.TGRU_encoder_cell2, concat_input, initializer=initial_hidden, name='encoder2_h')\n",
    "        return encoder2_h\n",
    "\n",
    "    # visit最后的输出h\n",
    "    def get_visit_h(self):  # Returns all hidden states for the samples in a batch\n",
    "\n",
    "        encoder2_h = self.get_encoder2_h()\n",
    "        encoder2_output = tf.map_fn(self.get_output2, encoder2_h)\n",
    "\n",
    "        batch_size = tf.shape(encoder2_h)[1]\n",
    "        scan_time = tf.transpose(self.time)  # scan_time [seq_length x batch_size]\n",
    "\n",
    "        # make info [batch_size x seq_length x info_dim] --> [seq_length x batch_size x info_dim]\n",
    "        scan_info = tf.transpose(self.info, [1, 0, 2])\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "\n",
    "        # [seq_length x batch_size x input_dim+info_dim+1]\n",
    "        concat_input = tf.concat([scan_time, scan_info, encoder2_output], 2)\n",
    "\n",
    "        # 已经知道了visit的个数和长度，可以直接把concat_input拆分成visit个\n",
    "        # [visit_num x visit_length x batch_size x input_dim+info_dim_1]\n",
    "        initial_hidden = tf.zeros([ batch_size, self.vhidden_dim], tf.float32)\n",
    "        concat_input = tf.reshape(concat_input, [self.visit_num, self.visit_length, tf.shape(concat_input)[1],\n",
    "                                                 tf.shape(concat_input)[2]])\n",
    "        visit_h = tf.scan(self.TGRU_visit_cell, concat_input, initializer=initial_hidden, name='visit')\n",
    "        final_h = tf.reverse(visit_h, [0])[0, :, :]\n",
    "        return final_h\n",
    "\n",
    "    def get_cross_loss(self):\n",
    "        output = self.get_visit_h()\n",
    "        final_output=self.get_output(output)\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.lable_multi_hot, logits=final_output))\n",
    "        \n",
    "        # 准确度\n",
    "        pre=tf.sigmoid(final_output)\n",
    "        dic=tf.abs(self.lable_multi_hot-pre)\n",
    "        \n",
    "        return cross_entropy, dic\n",
    "\n",
    "\n",
    "    def map_elapse_time(self, t, dim):\n",
    "        c1 = tf.constant(1, dtype=tf.float32)\n",
    "        c2 = tf.constant(2.7183, dtype=tf.float32)\n",
    "\n",
    "        T = tf.div(c1, tf.log(t + c2), name='Log_elapse_time')\n",
    "        # T = tf.div(c1, tf.add(t , c1), name='Log_elapse_time')\n",
    "        return T\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 获取数据集中病人的预测\n",
    "    def TGRU_visit_cell2(self, prev_h, concat_input):\n",
    "        # batch_size x input_dim+info_dim+info_dim+1\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "\n",
    "        # 时间\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "        # visit_info\n",
    "        info = tf.slice(concat_input, [0, 1], [batch_size, self.info_dim])\n",
    "        # 输入\n",
    "        x = tf.slice(concat_input, [0, 1+self.info_dim], [batch_size, self.output_dim2])\n",
    "\n",
    "        # 把输入和病人信息放在一个向量中\n",
    "        visit_x = tf.concat([x, info], 1)\n",
    "\n",
    "        # visit嵌入向量经过GRU输出\n",
    "        ft = self.map_elapse_time(t, self.vhidden_dim)\n",
    "        z = tf.sigmoid(tf.matmul(visit_x, self.Wz_v) + tf.matmul(prev_h, self.Uz_v) + self.bz_v)\n",
    "        r = tf.sigmoid(tf.matmul(visit_x, self.Wr_v) + tf.matmul(prev_h, self.Ur_v) + self.br_v)\n",
    "        d = tf.matmul(ft, self.Wd_v)\n",
    "        h_ = tf.multiply(d, prev_h)\n",
    "        h_canditate = tf.sigmoid(tf.matmul(visit_x, self.Wh_v) + tf.matmul(r * h_, self.Uh_v) + self.bh_v)\n",
    "        current_h = h_ - z * h_ + z * h_canditate\n",
    "\n",
    "        return current_h\n",
    "    # 备注的矩阵大小是假设一个用户的code有39个，2个visit，总共的code种类259个\n",
    "    def get_encoder1_h_forPre(self,no):  # Returns all hidden states for the samples in a batch\n",
    "        convert_input = self.get_input(tf.one_hot(indices=self.code_inputindex[no], depth=self.one_hot_input_dim,axis=1))\n",
    "        batch_size = 1\n",
    "\n",
    "        # (39, 1, 259)\n",
    "        scan_input = tf.transpose([convert_input], perm=[1, 0, 2])  # scan input is [seq_length x batch_size x input_dim]\n",
    "        # (39, 1)\n",
    "        scan_time = tf.transpose([tf.to_float(self.code_time[no])])  # scan_time [seq_length x batch_size]\n",
    "        # (1, 150)\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim1], tf.float32)\n",
    "        # (39, 1, 1) make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        # (39, 1, 260)\n",
    "        concat_input = tf.concat([scan_time, scan_input], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder1_h = tf.scan(self.TGRU_encoder_cell1, concat_input, initializer=initial_hidden, name='encoder1_h_forVector')\n",
    "        return encoder1_h\n",
    "\n",
    "    def get_encoder2_h_forPre(self,no):  # Returns all hidden states for the samples in a batch\n",
    "        # (39, 1, 150)\n",
    "        encoder1_h = self.get_encoder1_h_forPre(no)\n",
    "        # (39, 1, 150)\n",
    "        encoder1_output = tf.map_fn(self.get_output1, encoder1_h)\n",
    "        batch_size = 1\n",
    "        # (39, 1)\n",
    "        scan_time = tf.transpose([tf.to_float(self.code_time[no])])  # scan_time [seq_length x batch_size]\n",
    "        # (1, 150)\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim2], tf.float32)\n",
    "        # (39, 1, 1) # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        # (39, 1, 151)\n",
    "        concat_input = tf.concat([scan_time, encoder1_output], 2)  # [seq_length x batch_size x input_dim+1]\n",
    "\n",
    "        encoder2_h = tf.scan(self.TGRU_encoder_cell2, concat_input, initializer=initial_hidden, name='encoder2_h_forVector')\n",
    "        return encoder2_h\n",
    "\n",
    "    def get_one_visit_Pre(self,no):\n",
    "        encoder2_h = self.get_encoder2_h_forPre(no)\n",
    "        encoder2_output = tf.map_fn(self.get_output2, encoder2_h)\n",
    "        batch_size= 1\n",
    "        n = 0\n",
    "        x=[]\n",
    "        for i in range(self.patient_visit_num[no][0]):\n",
    "            #(1, 150)\n",
    "            visit_input=tf.zeros([batch_size, self.output_dim2], dtype=tf.float32)\n",
    "            for j in range(self.patient_visit_length[no][i]):\n",
    "                visit_input+=encoder2_output[n]\n",
    "                n=n+1\n",
    "            x.append(visit_input/self.patient_visit_length[no][i])\n",
    "        # (2, 1)\n",
    "        scan_time = tf.transpose([tf.to_float(self.visit_time[no])])  # scan_time [seq_length x batch_size]\n",
    "        # (2, 1, 2) make info [batch_size x seq_length x info_dim] --> [seq_length x batch_size x info_dim]\n",
    "        scan_info = tf.transpose(tf.to_float([self.visit_info[no]]), [1, 0, 2])\n",
    "        # (2, 1, 1)make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
    "        # (2, 1, 153)[seq_length x batch_size x input_dim+info_dim+1]\n",
    "        concat_input = tf.concat([scan_time, scan_info, x], 2)\n",
    "        # (1,100)\n",
    "        initial_hidden = tf.zeros([batch_size, self.vhidden_dim], tf.float32)\n",
    "\n",
    "        visit = tf.scan(self.TGRU_visit_cell2, concat_input, initializer=initial_hidden, name='visit')\n",
    "        visit_final_h = tf.reverse(visit,[0])[0, :, :]\n",
    "        return encoder2_output, visit_final_h\n",
    "\n",
    "    def get_one_pre(self, h):\n",
    "        output = tf.nn.relu(tf.matmul(h, self.Wo) + self.bo)\n",
    "        pre = tf.matmul(output, self.Wf) + self.bf\n",
    "        return pre\n",
    "    \n",
    "\n",
    "    def get_all_Pre_sigmoid(self):\n",
    "        pres=[]\n",
    "        attentions=[]\n",
    "        for no in range(self.patient_num):\n",
    "            encoder2_output , visit_final_h =self.get_one_visit_Pre(no)\n",
    "            attentions.append(encoder2_output)\n",
    "            pre=tf.sigmoid(self.get_one_pre(visit_final_h))\n",
    "            pres.append(pre)\n",
    "        return pres,attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch生成器\n",
    "# bacth中有cutting和padding，由于长度不同，因此开始学习的时候可以每个数据单独学习\n",
    "# 生成的batch里的病人visit和code的个数一样\n",
    "def batch_generator(outFile, visit_num, code_num, patient_num, batch_size, icd9_num):\n",
    "    patient_code_file=open(outFile + '/patient_code' + '.seqs','rb')\n",
    "    patient_code=pickle.load(patient_code_file)\n",
    "    \n",
    "    patient_visit_code_file=open(outFile + '/newSeqs' + '.seqs','rb')\n",
    "    patient_visit_code=pickle.load(patient_visit_code_file)\n",
    "    \n",
    "    codespatientsinfo_file=open(outFile + '/codespatientsinfo' + '.seqs','rb')\n",
    "    codespatientsinfo=pickle.load(codespatientsinfo_file)\n",
    "    \n",
    "    visit_delt_dates_file=open(outFile + '/visit_delt_dates' + '.seqs','rb')\n",
    "    visit_delt_dates=pickle.load(visit_delt_dates_file)\n",
    "    \n",
    "    code_delt_dates_file=open(outFile + '/code_delt_dates' + '.seqs','rb')\n",
    "    code_delt_dates=pickle.load(code_delt_dates_file)\n",
    "    \n",
    "    visits_num_file=open(outFile + '/visits_num' + '.seqs','rb')\n",
    "    visits_num=pickle.load(visits_num_file)\n",
    "    \n",
    "    codes_num_file=open(outFile + '/codes_num' + '.seqs','rb')\n",
    "    codes_num=pickle.load(codes_num_file)\n",
    "    \n",
    "    patient_code_file.close()\n",
    "    patient_visit_code_file.close()\n",
    "    codespatientsinfo_file.close()\n",
    "    visit_delt_dates_file.close()\n",
    "    code_delt_dates_file.close()\n",
    "    visits_num_file.close()\n",
    "    codes_num_file.close()\n",
    "\n",
    "    \n",
    "    batch_patient_code=[]\n",
    "    batch_code_delt_dates=[]\n",
    "    batch_codespatientsinfo=[]\n",
    "    batch_lable=[]\n",
    "    batch_lable_multi_hot=[]\n",
    "    \n",
    "    # padding and cutting\n",
    "    for i in range(batch_size):\n",
    "        j=random.randint(0, patient_num-1)\n",
    "        if visits_num[j][0]== visit_num:\n",
    "            #print 'pick',j\n",
    "            code=[]\n",
    "            code_date=[]\n",
    "            code_info=[]\n",
    "            for k in range(visit_num-1):\n",
    "                if  codes_num[j][k]> code_num:\n",
    "                    if k==0: \n",
    "                        code.extend(patient_code[j][0:code_num])\n",
    "                        code_date.extend(code_delt_dates[j][0:code_num])\n",
    "                        code_info.extend(codespatientsinfo[j][0:code_num])\n",
    "                    else:\n",
    "                        start=k*codes_num[j][k-1]\n",
    "                        code.extend(patient_code[j][start :start+code_num])\n",
    "                        code_date.extend(code_delt_dates[j][start:start+code_num])\n",
    "                        code_info.extend(codespatientsinfo[j][start:start+code_num])\n",
    "                elif codes_num[j][k] < code_num:\n",
    "                    if k==0:\n",
    "                        code.extend(patient_code[j][:codes_num[j][k]])\n",
    "                        code.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_date.extend(code_delt_dates[j][:codes_num[j][k]])\n",
    "                        code_date.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_info.extend(codespatientsinfo[j][:codes_num[j][k]])\n",
    "                        code_info.extend([[0,0]]*(code_num-codes_num[j][k]))\n",
    "                    else:\n",
    "                        start2=0\n",
    "                        for n in range(k):\n",
    "                            start2+=codes_num[j][n]\n",
    "                        code.extend(patient_code[j][start2: start2+codes_num[j][k]])\n",
    "                        code.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_date.extend(code_delt_dates[j][start2:start2+codes_num[j][k]])\n",
    "                        code_date.extend([0]*(code_num-codes_num[j][k]))\n",
    "                        code_info.extend(codespatientsinfo[j][start2:start2+codes_num[j][k]])\n",
    "                        code_info.extend([[0,0]]*(code_num-codes_num[j][k]))\n",
    "                else:\n",
    "                    code.extend(patient_code[j])\n",
    "                    code_date.extend(code_delt_dates[j])\n",
    "                    code_info.extend(codespatientsinfo[j])\n",
    "            batch_patient_code.append(code)\n",
    "            batch_code_delt_dates.append(code_date)  \n",
    "            batch_codespatientsinfo.append(code_info)\n",
    "            # 获取最后一个visit作为预测任务\n",
    "            f=visit_num-1\n",
    "            batch_lable.append(patient_visit_code[j][f]) #lable\n",
    "\n",
    "            # muti-hot lable\n",
    "    for labeli in batch_lable:\n",
    "        lable_multi_hot=[0]*icd9_num\n",
    "        for j in labeli:\n",
    "            lable_multi_hot[j]=1.0\n",
    "        batch_lable_multi_hot.append(lable_multi_hot) \n",
    "    \n",
    "        \n",
    "        # 考虑随机生成的时候可能有的数据始终选不到，因此还要有一个顺序生成\n",
    "    return batch_patient_code, batch_codespatientsinfo, batch_code_delt_dates,batch_lable_multi_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient_code_pre_split(outFile,DATA_file):\n",
    "    # 数据集获取\n",
    "    patient_code_file = open(outFile + '/patient_code' + '.seqs', 'rb')\n",
    "    patient_code = pickle.load(patient_code_file)\n",
    "\n",
    "    code_delt_dates_file = open(outFile + '/code_delt_dates' + '.seqs', 'rb')\n",
    "    code_delt_dates = pickle.load(code_delt_dates_file)\n",
    "\n",
    "    visit_delt_dates_file = open(outFile + '/visit_delt_dates' + '.seqs', 'rb')\n",
    "    visit_delt_dates = pickle.load(visit_delt_dates_file)\n",
    "\n",
    "    visits_num_file = open(outFile + '/visits_num' + '.seqs', 'rb')\n",
    "    visits_num = pickle.load(visits_num_file)\n",
    "\n",
    "    codes_num_file = open(outFile + '/codes_num' + '.seqs', 'rb')\n",
    "    codes_num = pickle.load(codes_num_file)\n",
    "\n",
    "    all_codes_num_file = open(outFile + '/all_codes_num' + '.seqs', 'rb')\n",
    "    all_codes_num = pickle.load(all_codes_num_file)\n",
    "\n",
    "    visitspatientsinfo_file = open(outFile + '/visitspatientsinfo' + '.seqs', 'rb')\n",
    "    visitspatientsinfo = pickle.load(visitspatientsinfo_file)\n",
    "\n",
    "    patient_code_part1=[]\n",
    "    patient_code_part2=[]\n",
    "    codes_num_part1=[]\n",
    "    codes_num_part2=[]\n",
    "    visits_num_part1=[]\n",
    "    visit_delt_dates_part1=[]\n",
    "    visitspatientsinfo_part1=[]\n",
    "    code_delt_dates_part1=[]\n",
    "    \n",
    "    for i in range(len(codes_num)):\n",
    "        patient_code_part1.append(patient_code[i][:(all_codes_num[i]-codes_num[i][-1])])\n",
    "        patient_code_part2.append(patient_code[i][all_codes_num[i] - (codes_num[i][-1]):])\n",
    "        code_delt_dates_part1.append(code_delt_dates[i][:(all_codes_num[i]-codes_num[i][-1])])\n",
    "        visits_num_part1.append([visits_num[i][0]-1])\n",
    "        visit_delt_dates_part1.append(visit_delt_dates[i][:-1])\n",
    "        visitspatientsinfo_part1.append(visitspatientsinfo[i][:-1])\n",
    "        codes_num_part1.append(codes_num[i][:-1])\n",
    "        codes_num_part2.append(codes_num[i][-1])\n",
    "        \n",
    "    # lable的one-hot\n",
    "    lables_multi_hot=[]\n",
    "    for labeli in patient_code_part2:\n",
    "        lable_multi_hot=[0]*icd9_num\n",
    "        for j in labeli:\n",
    "            lable_multi_hot[j]=1\n",
    "        lables_multi_hot.append(lable_multi_hot)     \n",
    "    \n",
    "    # 保存数据\n",
    "    pickle.dump(patient_code_part1, open(DATA_file + '/patient_code_part1' + '.seqs', 'wb'), -1) \n",
    "    pickle.dump(patient_code_part2, open(DATA_file + '/patient_code_part2' + '.seqs', 'wb'), -1) \n",
    "    pickle.dump(codes_num_part1, open(DATA_file + '/codes_num_part1' + '.seqs', 'wb'), -1) \n",
    "    pickle.dump(codes_num_part2, open(DATA_file + '/codes_num_part2' + '.seqs', 'wb'), -1) \n",
    "    pickle.dump(lables_multi_hot, open(DATA_file + '/lables_multi_hot' + '.seqs', 'wb'), -1) \n",
    "    pickle.dump(code_delt_dates_part1, open(DATA_file + '/code_delt_dates_part1' + '.seqs', 'wb'), -1) \n",
    "    pickle.dump(visits_num_part1, open(DATA_file + '/visits_num_part1' + '.seqs', 'wb'), -1) \n",
    "    pickle.dump(visit_delt_dates_part1, open(DATA_file + '/visit_delt_dates_part1' + '.seqs', 'wb'), -1) \n",
    "    pickle.dump(visitspatientsinfo_part1, open(DATA_file + '/visitspatientsinfo_part1' + '.seqs', 'wb'), -1) \n",
    "    \n",
    "\n",
    "    return patient_code_part1,patient_code_part2,codes_num_part1,codes_num_part2,lables_multi_hot,code_delt_dates_part1,visits_num_part1,visit_delt_dates_part1,visitspatientsinfo_part1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "learning_rate = 1e-3\n",
    "iters = 20\n",
    "dropout=1.0\n",
    "\n",
    "# 网络参数\n",
    "info_dim=2\n",
    "icd9_num=259\n",
    "one_hot_input_dim=icd9_num # 数据len(types)=259, one-hot的长度应该是259\n",
    "input_dim = 200 \n",
    "hidden_dim1 =150\n",
    "output_dim1=150\n",
    "hidden_dim2=150\n",
    "output_dim2=150\n",
    "vhidden_dim=100\n",
    "voutput_dim=150\n",
    "output_dim=200\n",
    "\n",
    "# 生成batch数据参数\n",
    "outFile='SEQ'\n",
    "checkpoint_dir_pre='MODEL_pre'\n",
    "result_dir='RESULT'\n",
    "batch_size=4\n",
    "visit_num=2 #visit_num=random.randint() \n",
    "code_num=10 #code_num=random.randint() # code_num=visit_length\n",
    "patient_num=14 #病人的总个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把数据集中一个病人的visit分开，用来预测\n",
    "patient_code_part1,patient_code_part2,codes_num_part1,codes_num_part2,lables_multi_hot,code_delt_dates_part1,visits_num_part1,visit_delt_dates_part1,visitspatientsinfo_part1=patient_code_pre_split('SEQ','PRE_SEQ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化\n",
    "mtgruprecode=MTGRU_precode(visit_num-1, code_num,one_hot_input_dim,input_dim,info_dim,output_dim1,output_dim2,voutput_dim,output_dim,vhidden_dim, hidden_dim1, hidden_dim2,  visits_num_part1,codes_num_part1,visit_delt_dates_part1,visitspatientsinfo_part1,patient_num,patient_code_part1,code_delt_dates_part1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-f40cbae30380>:313: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From <ipython-input-2-f40cbae30380>:231: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/sunchenxi/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py:180: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# 目标\n",
    "loss,dic = mtgruprecode.get_cross_loss()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多次走iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess= tf.Session() \n",
    "sess.run(init)\n",
    "Loss = np.zeros(iters)\n",
    "Acc=np.zeros(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss, Acc: 0.025130 0.908333\n",
      "Loss, Acc: 0.014639 0.975000\n",
      "Loss, Acc: 0.024474 0.908333\n",
      "Loss, Acc: 0.024726 0.891667\n",
      "Loss, Acc: 0.010892 0.816667\n",
      "Loss, Acc: 0.028854 0.758333\n",
      "Loss, Acc: 0.017366 0.875000\n",
      "Loss, Acc: 0.023441 0.858333\n",
      "Loss, Acc: 0.017876 0.858333\n",
      "Loss, Acc: 0.016964 0.858333\n",
      "Loss, Acc: 0.012866 0.933333\n",
      "Loss, Acc: 0.032120 0.741667\n",
      "Loss, Acc: 0.018755 0.900000\n",
      "Loss, Acc: 0.023540 0.891667\n",
      "Loss, Acc: 0.018706 0.833333\n",
      "Loss, Acc: 0.016990 0.900000\n",
      "Loss, Acc: 0.010323 0.975000\n",
      "Loss, Acc: 0.017528 0.941667\n",
      "Loss, Acc: 0.021634 0.916667\n",
      "Loss, Acc: 0.014494 0.975000\n"
     ]
    }
   ],
   "source": [
    "# 生成batch训练\n",
    "for i in range(iters):\n",
    "    LOSS = 0 #cross-loss\n",
    "    ACC=0 #对所有标签预测对的准确度\n",
    "    \n",
    "    for i in range(10): #学习完整个数据库需要的次数,要按照实际\n",
    "        # 生成batch\n",
    "        xindex, info, t,lable_multi_hot = batch_generator(outFile, visit_num, code_num, patient_num,batch_size,icd9_num)\n",
    "        if len(xindex)==0: continue\n",
    "        _, L,D = sess.run([optimizer, loss, dic], feed_dict={mtgruprecode.inputindex: xindex, mtgruprecode.time: t, mtgruprecode.info:info,mtgruprecode.lable_multi_hot:lable_multi_hot,mtgruprecode.keep_prob:dropout})\n",
    "        LOSS += L\n",
    "        \n",
    "        T_num=0.0\n",
    "        for p in D:\n",
    "            T_num+=1\n",
    "            for value in p:\n",
    "                if value>0.5:\n",
    "                    T_num-=1\n",
    "                    break\n",
    "        acc=T_num/len(D)\n",
    "        ACC+=acc\n",
    "    Acc[i]=ACC/10    \n",
    "    \n",
    "    Loss[i] = LOSS / 10\n",
    "    \n",
    "    #print('Loss %f' %(Loss[i]))\n",
    "    print('Loss, Acc: %f %f'  %(Loss[i], Acc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-f40cbae30380>:352: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "pres,attention = sess.run(mtgruprecode.get_all_Pre_sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# 总数据集上全部标签预测对的准确度\n",
    "right_num=0.0\n",
    "for i in range(patient_num):\n",
    "    dic=np.abs(lables_multi_hot[i]-pres[i][0])\n",
    "    right_num+=1\n",
    "    for d in dic:\n",
    "        if d>0.5:\n",
    "            right_num-=1\n",
    "            break\n",
    "accuracy=right_num/patient_num\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MODEL_pre/model.ckpt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, checkpoint_dir_pre + '/model.ckpt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存预测\n",
    "result_file='RESULT_pre'\n",
    "pickle.dump(pres, open(result_file + '/pres' + '.seqs', 'wb'), -1) # 一个病人所有的code，不区分visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习判别阈值\n",
    "##### 对于每一个类来说找到一个阈值使得f2-score最好\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file='RESULT_pre'\n",
    "pres_file = open(result_file + '/pres' + '.seqs', 'rb')\n",
    "pres = pickle.load(pres_file)\n",
    "DATA_file='PRE_SEQ'\n",
    "lables_multi_hot_file = open(DATA_file + '/lables_multi_hot' + '.seqs', 'rb')\n",
    "labels=pickle.load(lables_multi_hot_file)\n",
    "icd9_num=259\n",
    "patient_num=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_threshold(pres,labels):\n",
    "    threshold=[0.5]*icd9_num\n",
    "    f2score=[0.0]*icd9_num\n",
    "    for i in range(icd9_num): #对每个类\n",
    "        for th in range(1,10): #每个类的可能的threshold\n",
    "            thres=th/10.0\n",
    "            TP=0.0\n",
    "            FP=0.0\n",
    "            FN=0.0\n",
    "            TN=0.0\n",
    "            for j in range(patient_num):\n",
    "                if labels[j][i] == 1 and pres[j][0][i] > thres: TP+=1 #TP \n",
    "                elif labels[j][i] == 0 and pres[j][0][i] > thres: FP+=1 #FP\n",
    "                #elif labels[j][i] == 0 and pres[j][0][i] < thres: TN+=1 #TN\n",
    "                elif labels[j][i] == 1 and pres[j][0][i] < thres: FN+=1 #FN\n",
    "            if TP==0 : \n",
    "                f2score[i]=1 \n",
    "                continue\n",
    "            prec= TP/(TP+FP)\n",
    "            recall=TP/(TP+FN)\n",
    "            score= 5*prec*recall/(4*prec+recall)\n",
    "            if score > f2score[i]: \n",
    "                threshold[i]=thres\n",
    "                f2score[i]=score\n",
    "    mean_f2score=np.mean(f2score)\n",
    "    print mean_f2score\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9603369634813734\n"
     ]
    }
   ],
   "source": [
    "threshold=learn_threshold(pres,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.428571428571\n"
     ]
    }
   ],
   "source": [
    "# 在新的阈值下，对于病人的所有预测都正确的准确度\n",
    "new_right_num=0.0\n",
    "for i in range(patient_num):\n",
    "    new_right_num+=1\n",
    "    for j in range(icd9_num):\n",
    "        if pres[i][0][j] > threshold[j] and labels [i][j]==1: continue \n",
    "        elif pres[i][0][j] < threshold[j] and labels [i][j]==0: continue \n",
    "        else: \n",
    "            new_right_num-=1\n",
    "            break\n",
    "new_accuracy=new_right_num/patient_num\n",
    "print new_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6, 0.1, 0.5, 0.1, 0.1, 0.1, 0.5, 0.1, 0.1, 0.5, 0.1, 0.4, 0.1, 0.5, 0.5, 0.1, 0.1, 0.1, 0.5, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.5, 0.5, 0.5, 0.5, 0.4, 0.6, 0.1, 0.5, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.5, 0.1, 0.5, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.1, 0.1, 0.1, 0.1, 0.7, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 0.7, 0.7, 0.7, 0.7, 0.1, 0.7, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.5, 0.5, 0.5, 0.5, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.5, 0.5, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.1, 0.1, 0.5, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.5, 0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "print threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.428571428571\n"
     ]
    }
   ],
   "source": [
    "# 在新的阈值下，对于病人的所有预测容错率n的准确度\n",
    "new_right_num2=0.0\n",
    "for i in range(patient_num):\n",
    "    n=0\n",
    "    new_right_num2+=1\n",
    "    for j in range(icd9_num):\n",
    "        if pres[i][0][j] > threshold[j] and labels [i][j]==1: continue \n",
    "        elif pres[i][0][j] < threshold[j] and labels [i][j]==0: continue \n",
    "        elif n!=0: n-=1\n",
    "        elif n==0: \n",
    "            new_right_num2-=1\n",
    "            break\n",
    "new_accuracy2=new_right_num2/patient_num\n",
    "print new_accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13127413127413126"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 容错\n",
    "34.0/259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code的长期影响，从code到visit的attention的值得到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个病人的每个visit下最高贡献的code，死亡病人每个visit下最高贡献code\n",
    "def PatientImpcode(codeAttention):\n",
    "    codes_num_file= open(outFile + '/codes_num' + '.seqs', 'rb')\n",
    "    codes_num=pickle.load(codes_num_file) \n",
    "    newSeqs_file= open(outFile + '/newSeqs' + '.seqs', 'rb')\n",
    "    newSeqs=pickle.load(newSeqs_file) \n",
    "    types_file= open(outFile + '/types2' + '.seqs', 'rb')\n",
    "    types=pickle.load(types_file) \n",
    "    type_pids_file= open(outFile + '/type_pids2' + '.seqs', 'rb')\n",
    "    type_pids=pickle.load(type_pids_file)\n",
    "    death_labels_file=open(outFile + '/death_labels' + '.seqs','rb')\n",
    "    death_labels=pickle.load(death_labels_file)\n",
    "    visits_num_file=open(outFile + '/visits_num' + '.seqs','rb')\n",
    "    patient_code_file=open(outFile + '/patient_code' + '.seqs','rb')\n",
    "    patient_code=pickle.load(patient_code_file)\n",
    "    visits_num=pickle.load(visits_num_file)\n",
    "    newSeqs_file.close()\n",
    "    types_file.close()\n",
    "    type_pids_file.close()\n",
    "    codes_num_file.close()\n",
    "    death_labels_file.close()\n",
    "    visits_num_file.close()\n",
    "    patient_code_file.close()\n",
    "\n",
    "    #获取每个patient下的code的attention,每个visit的attention\n",
    "    code_attentions_value=[]\n",
    "    for i in range(patient_num):\n",
    "        code_attention_value=[]\n",
    "        visit_attention_value=[]\n",
    "        n=0\n",
    "        for j in range(len(codes_num[i])-1):\n",
    "            av=[]\n",
    "            for k in range(codes_num[i][j]):\n",
    "                av.append(np.linalg.norm(codeAttention[i][n]))\n",
    "                n+=1\n",
    "            code_attention_value.append(av)\n",
    "        code_attentions_value.append(code_attention_value)\n",
    "       \n",
    "    # 对每个patient下的visit下code的attention大小进行排名,visit的attention大小排名\n",
    "    code_attentions_rank=[]\n",
    "    for i in range(patient_num):\n",
    "        code_attention_rank=[]\n",
    "        for j in range(len(codes_num[i])-1):\n",
    "            rank=np.argsort(code_attentions_value[i][j])\n",
    "            if j ==0 : code_attention_rank.append(rank[::-1])\n",
    "            else:  code_attention_rank.append(rank[::-1]+codes_num[i][j-1])\n",
    "        code_attentions_rank.append(code_attention_rank)\n",
    "\n",
    "    # 病人id对应的重要code的icd9编码\n",
    "    PidImpcodeMap={}\n",
    "    for i in range(len(code_attentions_rank)):\n",
    "        a=[]\n",
    "        impCode=[]\n",
    "        for j in range(len(code_attentions_rank[i])):\n",
    "            a.append(types[patient_code[i][code_attentions_rank[i][j][0]]])\n",
    "        impCode.append(a)\n",
    "        PidImpcodeMap[type_pids[i]]=impCode\n",
    "        \n",
    "    return PidImpcodeMap\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "PidImpcodeMap=PatientImpcode(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10059: [['D_571.5']],\n",
       " 10088: [['D_285.9', 'D_276.2']],\n",
       " 10094: [['D_244.9']],\n",
       " 10117: [['D_287.5']],\n",
       " 10119: [['D_276.6']],\n",
       " 10124: [['D_998.0']],\n",
       " 40124: [['D_162.8']],\n",
       " 40310: [['D_453.41']],\n",
       " 41795: [['D_458.29']],\n",
       " 41976: [['D_285.9',\n",
       "   'D_244.9',\n",
       "   'D_785.52',\n",
       "   'D_998.11',\n",
       "   'D_584.9',\n",
       "   'D_008.45',\n",
       "   'D_244.9',\n",
       "   'D_357.2',\n",
       "   'D_518.81',\n",
       "   'D_507.0',\n",
       "   'D_530.19',\n",
       "   'D_507.0',\n",
       "   'D_438.82',\n",
       "   'D_530.19']],\n",
       " 42135: [['D_572.2']],\n",
       " 42346: [['D_V10.83']],\n",
       " 43881: [['D_327.23']],\n",
       " 44083: [['D_272.4', 'D_V10.03']]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 长期影响的重要code\n",
    "PidImpcodeMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
